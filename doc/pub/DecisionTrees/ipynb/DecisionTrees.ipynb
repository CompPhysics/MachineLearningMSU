{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Data Analysis and Machine Learning: Trees, forests and all that -->\n",
    "# Data Analysis and Machine Learning: Trees, forests and all that\n",
    "<!-- dom:AUTHOR: Morten Hjorth-Jensen at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University -->\n",
    "<!-- Author: -->  \n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo and Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University\n",
    "\n",
    "Date: **May 19, 2019**\n",
    "\n",
    "Copyright 1999-2019, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Decision trees, overarching aims\n",
    "\n",
    "\n",
    "Decision trees are supervised learning algorithms used for both,\n",
    "classification and regression tasks.\n",
    "\n",
    "\n",
    "The main idea of decision trees\n",
    "is to find those descriptive features which contain the most\n",
    "**information** regarding the target feature and then split the dataset\n",
    "along the values of these features such that the target feature values\n",
    "for the resulting sub datasets are as pure as possible.\n",
    "\n",
    "The descriptive feature which leaves the target feature most purely is said\n",
    "to be the most informative one. This process of finding the **most\n",
    "informative** feature is done until we accomplish a stopping criteria\n",
    "where we then finally end up in so called **leaf nodes**. \n",
    "\n",
    "The leaf nodes\n",
    "contain the predictions we will make for new query instances presented\n",
    "to our trained model. This is possible since the model has kind of\n",
    "learned the underlying structure of the training data and hence can,\n",
    "given some assumptions, make predictions about the target feature value\n",
    "(class) of unseen query instances.\n",
    "\n",
    "\n",
    "A decision tree mainly contains of a **root node**, **interior nodes**,\n",
    "and **leaf nodes** which are then connected by **branches**.\n",
    "\n",
    "\n",
    "## How do we set it up?\n",
    "\n",
    "\n",
    "In simplified terms, the process of training a decision tree and\n",
    "predicting the target features of query instances is as follows:\n",
    "\n",
    "1. Present a dataset containing of a number of training instances characterized by a number of descriptive features and a target feature\n",
    "\n",
    "2. Train the decision tree model by continuously splitting the target feature along the values of the descriptive features using a measure of information gain during the training process\n",
    "\n",
    "3. Grow the tree until we accomplish a stopping criteria create leaf nodes which represent the *predictions* we want to make for new query instances\n",
    "\n",
    "4. Show query instances to the tree and run down the tree until we arrive at leaf nodes\n",
    "\n",
    "Then we are essentially done!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Decision trees and Regression, our Nuclear data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            N    Z    A Element  Ebinding   Eapprox\n",
      "A                                                  \n",
      "1   0       0    1    1       H  0.000000  0.000000\n",
      "2   1       1    1    2       H  1.112283  1.112283\n",
      "3   2       2    1    3       H  2.827265  2.699972\n",
      "    3       1    2    3      He  2.572680  2.699972\n",
      "4   5       3    1    4       H  1.720449  3.316041\n",
      "    6       2    2    4      He  7.073915  3.316041\n",
      "    7       1    3    4      Li  1.153760  3.316041\n",
      "5   8       4    1    5       H  1.336359  4.038208\n",
      "    9       3    2    5      He  5.512132  4.038208\n",
      "    10      2    3    5      Li  5.266132  4.038208\n",
      "6   12      5    1    6       H  0.961639  3.914934\n",
      "    13      4    2    6      He  4.878519  3.914934\n",
      "    14      3    3    6      Li  5.332331  3.914934\n",
      "    15      2    4    6      Be  4.487247  3.914934\n",
      "7   18      5    2    7      He  4.123057  4.664937\n",
      "    19      4    3    7      Li  5.606439  4.664937\n",
      "    20      3    4    7      Be  5.371548  4.664937\n",
      "    21      2    5    7       B  3.558705  4.664937\n",
      "8   22      6    2    8      He  3.924520  4.793069\n",
      "    23      5    3    8      Li  5.159712  4.793069\n",
      "    24      4    4    8      Be  7.062435  4.793069\n",
      "    25      3    5    8       B  4.717155  4.793069\n",
      "    26      2    6    8       C  3.101524  4.793069\n",
      "9   27      7    2    9      He  3.349037  5.088793\n",
      "    28      6    3    9      Li  5.037768  5.088793\n",
      "    29      5    4    9      Be  6.462668  5.088793\n",
      "    30      4    5    9       B  6.257070  5.088793\n",
      "    31      3    6    9       C  4.337423  5.088793\n",
      "10  32      8    2   10      He  2.995134  5.029152\n",
      "    33      7    3   10      Li  4.531351  5.029152\n",
      "...       ...  ...  ...     ...       ...       ...\n",
      "254 3224  156   98  254      Cf  7.449225  7.442482\n",
      "    3225  155   99  254      Es  7.443589  7.442482\n",
      "    3226  154  100  254      Fm  7.444792  7.442482\n",
      "    3228  152  102  254      No  7.423590  7.442482\n",
      "255 3232  156   99  255      Es  7.437821  7.424594\n",
      "    3233  155  100  255      Fm  7.435888  7.424594\n",
      "    3234  154  101  255      Md  7.428729  7.424594\n",
      "    3235  153  102  255      No  7.417958  7.424594\n",
      "    3236  152  103  255      Lr  7.402576  7.424594\n",
      "256 3241  156  100  256      Fm  7.431780  7.407882\n",
      "    3243  154  102  256      No  7.416546  7.407882\n",
      "    3244  153  103  256      Lr  7.398160  7.407882\n",
      "    3245  152  104  256      Rf  7.385434  7.407882\n",
      "257 3248  157  100  257      Fm  7.422194  7.407882\n",
      "    3249  156  101  257      Md  7.417582  7.407882\n",
      "    3250  155  102  257      No  7.409657  7.407882\n",
      "    3252  153  104  257      Rf  7.381704  7.407882\n",
      "258 3256  157  101  258      Md  7.409675  7.396107\n",
      "    3259  154  104  258      Rf  7.382538  7.396107\n",
      "259 3264  157  102  259      No  7.399974  7.380168\n",
      "    3267  154  105  259      Db  7.360362  7.380168\n",
      "260 3275  154  106  260      Sg  7.342562  7.342562\n",
      "261 3280  157  104  261      Rf  7.371384  7.355577\n",
      "    3282  155  106  261      Sg  7.339770  7.355577\n",
      "262 3289  156  106  262      Sg  7.341185  7.341185\n",
      "264 3304  156  108  264      Hs  7.298375  7.298375\n",
      "265 3310  157  108  265      Hs  7.296247  7.297260\n",
      "266 3317  158  108  266      Hs  7.298273  7.297260\n",
      "269 3338  159  110  269      Ds  7.250154  7.250154\n",
      "270 3344  160  110  270      Ds  7.253775  7.253775\n",
      "\n",
      "[2497 rows x 6 columns]\n",
      "0.08411726015367603\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Regression analysis using scikit-learn functions\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import os\n",
    "\n",
    "# Where to save the figures and data files\n",
    "PROJECT_ROOT_DIR = \"Results\"\n",
    "FIGURE_ID = \"Results/FigureFiles\"\n",
    "DATA_ID = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(PROJECT_ROOT_DIR):\n",
    "    os.mkdir(PROJECT_ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(FIGURE_ID):\n",
    "    os.makedirs(FIGURE_ID)\n",
    "\n",
    "if not os.path.exists(DATA_ID):\n",
    "    os.makedirs(DATA_ID)\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(FIGURE_ID, fig_id)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png')\n",
    "\n",
    "infile = open(data_path(\"MassEval2016.dat\"),'r')\n",
    "\n",
    "\n",
    "# Read the experimental data with Pandas\n",
    "Masses = pd.read_fwf(infile, usecols=(2,3,4,6,11),\n",
    "              names=('N', 'Z', 'A', 'Element', 'Ebinding'),\n",
    "              widths=(1,3,5,5,5,1,3,4,1,13,11,11,9,1,2,11,9,1,3,1,12,11,1),\n",
    "              header=39,\n",
    "              index_col=False)\n",
    "\n",
    "# Extrapolated values are indicated by '#' in place of the decimal place, so\n",
    "# the Ebinding column won't be numeric. Coerce to float and drop these entries.\n",
    "Masses['Ebinding'] = pd.to_numeric(Masses['Ebinding'], errors='coerce')\n",
    "Masses = Masses.dropna()\n",
    "# Convert from keV to MeV.\n",
    "Masses['Ebinding'] /= 1000\n",
    "\n",
    "# Group the DataFrame by nucleon number, A.\n",
    "Masses = Masses.groupby('A')\n",
    "# Find the rows of the grouped DataFrame with the maximum binding energy.\n",
    "Masses = Masses.apply(lambda t: t[t.Ebinding==t.Ebinding])\n",
    "A = Masses['A']\n",
    "Z = Masses['Z']\n",
    "N = Masses['N']\n",
    "Element = Masses['Element']\n",
    "Energies = Masses['Ebinding']\n",
    "# Now we set up the design matrix X\n",
    "X = np.zeros((1,len(A)))\n",
    "#X[4,:] = A**(-1.0)\n",
    "#X[3,:] = A**(-1.0/3.0)\n",
    "#X[2,:] = A**(2.0/3.0)\n",
    "X[0,:] = A\n",
    "#X[0,:] = 1\n",
    "\n",
    "\n",
    "#Decision Tree Regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regr_1=DecisionTreeRegressor(max_depth=5)\n",
    "regr_2=DecisionTreeRegressor(max_depth=7)\n",
    "regr_3=DecisionTreeRegressor(max_depth=9)\n",
    "regr_1.fit(X.T, Energies)\n",
    "regr_2.fit(X.T, Energies)\n",
    "regr_3.fit(X.T, Energies)\n",
    "\n",
    "\n",
    "y_1 = regr_1.predict(X.T)\n",
    "y_2 = regr_2.predict(X.T)\n",
    "y_3=regr_3.predict(X.T)\n",
    "Masses['Eapprox'] = y_3\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.plot(A, Energies, color=\"blue\", label=\"Data\", linewidth=2)\n",
    "plt.plot(A, y_1, color=\"red\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.plot(A, y_2, color=\"green\", label=\"max_depth=7\", linewidth=2)\n",
    "plt.plot(A, y_3, color=\"m\", label=\"max_depth=9\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"$A$\")\n",
    "plt.ylabel(\"$E$[MeV]\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "save_fig(\"Masses2016Trees\")\n",
    "plt.show()\n",
    "print(Masses)\n",
    "print(np.mean( (Energies-y_3)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a tree, regression\n",
    "\n",
    "There are mainly two steps\n",
    "1. We split the predictor space (the set of possible values $x_1,x_2,\\dots, x_p$) into $J$\n",
    "\n",
    "distinct and non-non-overlapping regions, $R_1,R_2,\\dots,R_J$.  \n",
    "1. For every observation that falls into the region $R_j$ , we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.\n",
    "\n",
    "How do we construct the regions $R_1,\\dots,R_J$? \n",
    "In theory, the regions could have any shape. However, we\n",
    "choose to divide the predictor space into high-dimensional rectangles,\n",
    "or boxes, for simplicity and for ease of interpretation of the\n",
    "resulting predic- tive model. The goal is to find boxes $R_1,\\dots,R_J$ \n",
    "that minimize the MSE, given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{j=1}^J\\sum_{i\\in R_j}(y_i-\\overline{y}_{R_j})^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\overline{y}_{R_j}$  is the mean response for the training observations \n",
    "within the $j$th\n",
    "box. \n",
    "\n",
    "## A top-down approach, recursive binary splitting\n",
    "\n",
    "Unfortunately, it is computationally infeasible to consider every\n",
    "possible partition of the feature space into $J$ boxes. \n",
    "The common strategy is to take a top-down approach\n",
    "\n",
    "The approach is top-down because it begins at the top of the tree (all\n",
    "observations belong to a single region) and then successively splits\n",
    "the predictor space; each split is indicated via two new branches\n",
    "further down on the tree. It is greedy because at each step of the\n",
    "tree-building process, the best split is made at that particular step,\n",
    "rather than looking ahead and picking a split that will lead to a\n",
    "better tree in some future step.\n",
    "\n",
    "## Making a tree\n",
    "\n",
    "In order to implement the recursive binary splitting we start by selecting\n",
    "the predictor $x_j$ and a cutpoint $s$ that splits the predictor space into two regions $R_1$ and $R_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\{X\\vert x_j < s\\right\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\{X\\vert x_j \\geq s\\right\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so that we obtain the lowest MSE, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{i:x_i\\in R_j}(y_i-\\overline{y}_{R_1})^2+\\sum_{i:x_i\\in R_2}(y_i-\\overline{y}_{R_2})^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we want to minimize by considering all predictors $x_1,x_2,\\dots,x_p$.\n",
    "We consider also all possible values of $s$ for each predictor. These values could be determined by randomly assigned numbers or by starting at the midpoint and then proceed till we find an optimal value.\n",
    "\n",
    "For any $j$ and $s$, we define the pair of\n",
    "half-planes where $\\overline{y}_{R_1}$ is the mean response for the training\n",
    "observations in $R_1(j,s)$, and $\\overline{y}_{R_2}$ is the mean response for the\n",
    "training observations in $R_2(j,s)$. \n",
    "\n",
    "Finding the values of j and s that\n",
    "minimize the above equation can be done quite quickly, especially when the number\n",
    "of features $p$ is not too large.  \n",
    "\n",
    "Next, we repeat the process, looking\n",
    "for the best predictor and best cutpoint in order to split the data\n",
    "further so as to minimize the MSE within each of the resulting\n",
    "regions. However, this time, instead of splitting the entire predictor\n",
    "space, we split one of the two previously identified regions. We now\n",
    "have three regions. Again, we look to split one of these three regions\n",
    "further, so as to minimize the MSE. The process continues until a\n",
    "stopping criterion is reached; for instance, we may continue until no\n",
    "region contains more than five observations.\n",
    "\n",
    "<!-- !split  -->\n",
    "## Pruning the tree\n",
    "\n",
    "The above procedure is rather straightforward, but leads often to\n",
    "overfitting and unnecessarily large and complicated trees. The basic\n",
    "idea is to grow a large tree $T_0$ and then prune it back in order to\n",
    "obtain a subtree. A smaller tree with fewer splits (fewer regions) can\n",
    "lead to smaller variance and better interpretation at the cost of a\n",
    "little more bias.\n",
    "\n",
    "The so-called Cost complexity pruning algorithm gives us a\n",
    "way to do just this. Rather than considering every possible subtree,\n",
    "we consider a sequence of trees indexed by a nonnegative tuning\n",
    "parameter $\\alpha$.\n",
    "\n",
    "## Cost complexity pruning\n",
    "For each value of $\\alpha$  there corresponds a subtree $T \\in T_0$ such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{m=1}^{\\overline{T}}\\sum_{i:x_i\\in R_m}(y_i-\\overline{y}_{R_m})^2+\\alpha\\overline{T},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is as small as possible. Here $\\overline{T}$ is \n",
    "the number of terminal nodes of the tree $T$ , $R_m$ is the\n",
    "rectangle (i.e. the subset of predictor space)  corresponding to the $m$-th terminal node.\n",
    "\n",
    "The tuning parameter $\\alpha$ controls a trade-off between the subtreeâ€™s\n",
    "com- plexity and its fit to the training data. When $\\alpha = 0$, then the\n",
    "subtree $T$ will simply equal $T_0$, \n",
    "because then the above equation just measures the\n",
    "training error. \n",
    "However, as $\\alpha$ increases, there is a price to pay for\n",
    "having a tree with many terminal nodes. The above equation will\n",
    "tend to be minimized for a smaller subtree. \n",
    "\n",
    "\n",
    "It turns out that as we increase $\\alpha$ from zero\n",
    "branches get pruned from the tree in a nested and predictable fashion,\n",
    "so obtaining the whole sequence of subtrees as a function of $\\alpha$ is\n",
    "easy. We can select a value of $\\alpha$ using a validation set or using\n",
    "cross-validation. We then return to the full data set and obtain the\n",
    "subtree corresponding to $\\alpha$. \n",
    "\n",
    "\n",
    "## A schematic procedure\n",
    "\n",
    "**Building a Regression Tree.**\n",
    "\n",
    "1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\n",
    "\n",
    "2. Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of $\\alpha$.\n",
    "\n",
    "3. Use for example $K$-fold cross-validation to choose $\\alpha$. Divide the training observations into $K$ folds. For each $k=1,2,\\dots,K$ we: \n",
    "\n",
    "  * repeat steps 1 and 2 on all but the $k$-th fold of the training data. \n",
    "\n",
    "  * Then we valuate the mean squared prediction error on the data in the left-out $k$-th fold, as a function of $\\alpha$.\n",
    "\n",
    "  * Finally  we average the results for each value of $\\alpha$, and pick $\\alpha$ to minimize the average error.\n",
    "\n",
    "\n",
    "4. Return the subtree from Step 2 that corresponds to the chosen value of $\\alpha$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## A classification tree\n",
    "\n",
    "A classification tree is very similar to a regression tree, except\n",
    "that it is used to predict a qualitative response rather than a\n",
    "quantitative one. Recall that for a regression tree, the predicted\n",
    "response for an observation is given by the mean response of the\n",
    "training observations that belong to the same terminal node. In\n",
    "contrast, for a classification tree, we predict that each observation\n",
    "belongs to the most commonly occurring class of training observations\n",
    "in the region to which it belongs. In interpreting the results of a\n",
    "classification tree, we are often interested not only in the class\n",
    "prediction corresponding to a particular terminal node region, but\n",
    "also in the class proportions among the training observations that\n",
    "fall into that region.  \n",
    "\n",
    "## Growing a classification tree\n",
    "\n",
    "The task of growing a\n",
    "classification tree is quite similar to the task of growing a\n",
    "regression tree. Just as in the regression setting, we use recursive\n",
    "binary splitting to grow a classification tree. However, in the\n",
    "classification setting, the MSE cannot be used as a criterion for making\n",
    "the binary splits.  A natural alternative to MSE is the **classification\n",
    "error rate**. Since we plan to assign an observation in a given region\n",
    "to the most commonly occurring error rate class of training\n",
    "observations in that region, the classification error rate is simply\n",
    "the fraction of the training observations in that region that do not\n",
    "belong to the most common class. \n",
    "\n",
    "When building a classification tree, either the Gini index or the\n",
    "entropy are typically used to evaluate the quality of a particular\n",
    "split, since these two approaches are more sensitive to node purity\n",
    "than is the classification error rate. \n",
    "\n",
    "\n",
    "## Classification tree, how to split nodes\n",
    "If our targets are the outcome of a classification process that takes for example \n",
    "$k=1,2,\\dots,K$ values, the only thing we need to think of is to set up the splitting criteria for each node.\n",
    "\n",
    "We define a PDF $p_{mk}$  that represents the number of observations of a class $k$ in a region $R_m$ with $N_m$ observations. We represent this likelihood function in terms of the proportion $I(y_i=k)$  of observations of this class in the region $R_m$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p_{mk} = \\frac{1}{N_m}\\sum_{x_i\\in R_m}I(y_i=k).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We let $p_{mk}$ represent the majority class of observations in region $m$. The three most common ways of splitting a node are given by \n",
    "* Misclassification error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p_{mk} = \\frac{1}{N_m}\\sum_{x_i\\in R_m}I(y_i\\ne k) = 1-p_{mk}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gini index $g$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g = \\sum_{k=1}^K p_{mk}(1-p_{mk}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Information entropy or just entropy $s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s = -\\sum_{k=1}^K p_{mk}\\log{p_{mk}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to moons again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=53)\n",
    "\n",
    "deep_tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
    "deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)\n",
    "deep_tree_clf1.fit(Xm, ym)\n",
    "deep_tree_clf2.fit(Xm, ym)\n",
    "\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if not iris:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    if plot_training:\n",
    "        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\n",
    "        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\n",
    "        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Iris-Virginica\")\n",
    "        plt.axis(axes)\n",
    "    if iris:\n",
    "        plt.xlabel(\"Petal length\", fontsize=14)\n",
    "        plt.ylabel(\"Petal width\", fontsize=14)\n",
    "    else:\n",
    "        plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "    if legend:\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n",
    "plt.title(\"No restrictions\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n",
    "plt.title(\"min_samples_leaf = {}\".format(deep_tree_clf2.min_samples_leaf), fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "Xs = np.random.rand(100, 2) - 0.5\n",
    "ys = (Xs[:, 0] > 0).astype(np.float32) * 2\n",
    "\n",
    "angle = np.pi / 4\n",
    "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "Xsr = Xs.dot(rotation_matrix)\n",
    "\n",
    "tree_clf_s = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_s.fit(Xs, ys)\n",
    "tree_clf_sr = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_sr.fit(Xsr, ys)\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(tree_clf_s, Xs, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(tree_clf_sr, Xsr, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic training set + noise\n",
    "np.random.seed(42)\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final regressor code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
    "    y_pred = tree_reg.predict(x1)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel, fontsize=18, rotation=0)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.subplot(121)\n",
    "plot_regression_predictions(tree_reg1, X, y)\n",
    "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "plt.text(0.21, 0.65, \"Depth=0\", fontsize=15)\n",
    "plt.text(0.01, 0.2, \"Depth=1\", fontsize=13)\n",
    "plt.text(0.65, 0.8, \"Depth=1\", fontsize=13)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"max_depth=2\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_regression_predictions(tree_reg2, X, y, ylabel=None)\n",
    "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "for split in (0.0458, 0.1298, 0.2873, 0.9040):\n",
    "    plt.plot([split, split], [-0.2, 1], \"k:\", linewidth=1)\n",
    "plt.text(0.3, 0.5, \"Depth=2\", fontsize=13)\n",
    "plt.title(\"max_depth=3\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "x1 = np.linspace(0, 1, 500).reshape(-1, 1)\n",
    "y_pred1 = tree_reg1.predict(x1)\n",
    "y_pred2 = tree_reg2.predict(x1)\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", fontsize=18, rotation=0)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"No restrictions\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.title(\"min_samples_leaf={}\".format(tree_reg2.min_samples_leaf), fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and cons of trees, pros\n",
    "\n",
    "* White box, easy to interpret model. Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches discussed earlier (think of support vector machines)\n",
    "\n",
    "* Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!\n",
    "\n",
    "* No feature normalization needed\n",
    "\n",
    "* Tree models can handle both continuous and categorical data (Classification and Regression Trees)\n",
    "\n",
    "* Can model nonlinear relationships\n",
    "\n",
    "* Can model interactions between the different descriptive features\n",
    "\n",
    "* Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small)\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "* Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches\n",
    "\n",
    "* If continuous features are used the tree may become quite large and hence less interpretable\n",
    "\n",
    "* Decision trees are prone to overfit the training data and hence do not well generalize the data if no stopping criteria or improvements like pruning, boosting or bagging are implemented\n",
    "\n",
    "* Small changes in the data may lead to a completely different tree. This issue can be addressed by using ensemble methods like bagging, boosting or random forests\n",
    "\n",
    "* Unbalanced datasets where some target feature values occur much more frequently than others may lead to biased trees since the frequently occurring feature values are preferred over the less frequently occurring ones. \n",
    "\n",
    "* If the number of features is relatively large (high dimensional) and the number of instances is relatively low, the tree might overfit the data\n",
    "\n",
    "* Features with many levels may be preferred over features with less levels since for them it is *more easy* to split the dataset such that the sub datasets only contain pure target feature values. This issue can be addressed by preferring for instance the information gain ratio as splitting criteria over information gain\n",
    "\n",
    "However, by aggregating many decision trees, using methods like bagging, random forests, and boosting, the predictive performance of trees can be substantially improved. \n",
    "\n",
    "## Bagging\n",
    "\n",
    "The **plain** decision trees suffer from high\n",
    "variance. This means that if we split the training data into two parts\n",
    "at random, and fit a decision tree to both halves, the results that we\n",
    "get could be quite different. In contrast, a procedure with low\n",
    "variance will yield similar results if applied repeatedly to distinct\n",
    "data sets; linear regression tends to have low variance, if the ratio\n",
    "of $n$ to $p$ is moderately large. \n",
    "\n",
    "**Bootstrap aggregation**, or just **bagging**, is a\n",
    "general-purpose procedure for reducing the variance of a statistical\n",
    "learning method. \n",
    "\n",
    "\n",
    "Bagging typically results in improved accuracy\n",
    "over prediction using a single tree. Unfortunately, however, it can be\n",
    "difficult to interpret the resulting model. Recall that one of the\n",
    "advantages of decision trees is the attractive and easily interpreted\n",
    "diagram that results.\n",
    "\n",
    "However, when we bag a large number of trees, it is no longer\n",
    "possible to represent the resulting statistical learning procedure\n",
    "using a single tree, and it is no longer clear which variables are\n",
    "most important to the procedure. Thus, bagging improves prediction\n",
    "accuracy at the expense of interpretability.  Although the collection\n",
    "of bagged trees is much more difficult to interpret than a single\n",
    "tree, one can obtain an overall summary of the importance of each\n",
    "predictor using the MSE (for bagging regression trees) or the Gini\n",
    "index (for bagging classification trees). In the case of bagging\n",
    "regression trees, we can record the total amount that the MSE is\n",
    "decreased due to splits over a given predictor, averaged over all $B$ possible\n",
    "trees. A large value indicates an important predictor. Similarly, in\n",
    "the context of bagging classification trees, we can add up the total\n",
    "amount that the Gini index  is decreased by splits over a given\n",
    "predictor, averaged over all $B$ trees.\n",
    "\n",
    "## Random forests\n",
    "\n",
    "Random forests provide an improvement over bagged trees by way of a\n",
    "small tweak that decorrelates the trees. \n",
    "\n",
    "As in bagging, we build a\n",
    "number of decision trees on bootstrapped training samples. But when\n",
    "building these decision trees, each time a split in a tree is\n",
    "considered, a random sample of $m$ predictors is chosen as split\n",
    "candidates from the full set of $p$ predictors. The split is allowed to\n",
    "use only one of those $m$ predictors. \n",
    "\n",
    "A fresh sample of $m$ predictors is\n",
    "taken at each split, and typically we choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "m\\approx \\sqrt{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In building a random forest, at\n",
    "each split in the tree, the algorithm is not even allowed to consider\n",
    "a majority of the available predictors. \n",
    "\n",
    "The reason for this is rather clever. Suppose that there is one very\n",
    "strong predictor in the data set, along with a number of other\n",
    "moderately strong predictors. Then in the collection of bagged\n",
    "variable importance random forest trees, most or all of the trees will\n",
    "use this strong predictor in the top split. Consequently, all of the\n",
    "bagged trees will look quite similar to each other. Hence the\n",
    "predictions from the bagged trees will be highly correlated. \n",
    "Unfortunately, averaging many highly correlated quantities does not lead\n",
    "to as large of a reduction in variance as averaging many uncorrelated\n",
    "quanti- ties. In particular, this means that bagging will not lead to\n",
    "a substantial reduction in variance over a single tree in this\n",
    "setting.  \n",
    "\n",
    "## A simple scikit-learn example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "# Data set not specificied\n",
    "X = dataset.XXX\n",
    "Y = dataset.YYY\n",
    "#Instantiate the model with 100 trees and entropy as splitting criteria\n",
    "Random_Forest_model = RandomForestClassifier(n_estimators=100,criterion=\"entropy\")\n",
    "#Cross validation\n",
    "accuracy = cross_validate(Random_Forest_model,X,Y,cv=10)['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please, not the moons again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "svm_clf = SVC(random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(random_state=42)\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "svm_clf = SVC(probability=True, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "plt.title(\"Decision Tree\", fontsize=14)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(bag_clf, X, y)\n",
    "plt.title(\"Decision Trees with Bagging\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=42),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "np.sum(y_pred == y_pred_rf) / len(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
