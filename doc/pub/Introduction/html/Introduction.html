<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Data Analysis and Machine Learning: Getting started, our first data and Machine Learning encounters">

<title>Data Analysis and Machine Learning: Getting started, our first data and Machine Learning encounters</title>


<style type="text/css">
/* bloodish style */

body {
  font-family: Helvetica, Verdana, Arial, Sans-serif;
  color: #404040;
  background: #ffffff;
}
h1 { font-size: 1.8em;  color: #8A0808; }
h2 { font-size: 1.6em;  color: #8A0808; }
h3 { font-size: 1.4em;  color: #8A0808; }
h4 { color: #8A0808; }
a { color: #8A0808; text-decoration:none; }
tt { font-family: "Courier New", Courier; }
/* pre style removed because it will interfer with pygments */
p { text-indent: 0px; }
hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
p.caption { width: 80%; font-style: normal; text-align: left; }
hr.figure { border: 0; width: 80%; border-bottom: 1px solid #aaa}
.alert-text-small   { font-size: 80%;  }
.alert-text-large   { font-size: 130%; }
.alert-text-normal  { font-size: 90%;  }
.alert {
  padding:8px 35px 8px 14px; margin-bottom:18px;
  text-shadow:0 1px 0 rgba(255,255,255,0.5);
  border:1px solid #bababa;
  border-radius: 4px;
  -webkit-border-radius: 4px;
  -moz-border-radius: 4px;
  color: #555;
  background-color: #f8f8f8;
  background-position: 10px 5px;
  background-repeat: no-repeat;
  background-size: 38px;
  padding-left: 55px;
  width: 75%;
 }
.alert-block {padding-top:14px; padding-bottom:14px}
.alert-block > p, .alert-block > ul {margin-bottom:1em}
.alert li {margin-top: 1em}
.alert-block p+p {margin-top:5px}
.alert-notice { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_gray_notice.png); }
.alert-summary  { background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_gray_summary.png); }
.alert-warning { background-image: url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_gray_warning.png); }
.alert-question {background-image:url(https://cdn.rawgit.com/hplgit/doconce/master/bundled/html_images/small_gray_question.png); }

div { text-align: justify; text-justify: inter-word; }
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Introduction', 2, None, '___sec0'),
              ('Software and needed installations', 2, None, '___sec1'),
              ('Python installers', 2, None, '___sec2'),
              ('Useful Python packages', 2, None, '___sec3'),
              ('Installing R, C++, cython or Julia', 2, None, '___sec4'),
              ('Installing R, C++, cython, Numba etc', 2, None, '___sec5'),
              ('Numpy examples and Important Matrix and vector handling '
               'packages',
               2,
               None,
               '___sec6'),
              ('Basic Matrix Features', 2, None, '___sec7'),
              ('Basic Matrix Features', 2, None, '___sec8'),
              ('Basic Matrix Features', 2, None, '___sec9'),
              ('Some famous Matrices', 2, None, '___sec10'),
              ('Basic Matrix Features', 2, None, '___sec11'),
              ('Numpy and arrays', 2, None, '___sec12'),
              ('Matrices in Python', 2, None, '___sec13'),
              ('Reading Data', 2, None, '___sec14'),
              ('Simple linear regression model using _scikit-learn_',
               2,
               None,
               '___sec15'),
              ('Random walk model', 3, None, '___sec16'),
              ('Concluding with another teaser, $k$-nearest neighbors with '
               'scikit-learn',
               2,
               None,
               '___sec17')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- ------------------- main content ---------------------- -->



<center><h1>Data Analysis and Machine Learning: Getting started, our first data and Machine Learning encounters</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>

<p>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, University of Oslo</b></center>
<center>[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>
<center><h4>Mar 22, 2019</h4></center> <!-- date -->
<br>

<h2 id="___sec0">Introduction </h2>

<p>
Our emphasis throughout this series of lectures  
is on understanding the mathematical aspects of
different algorithms used in the fields of data analysis and machine learning.

<p>
However, where possible we will emphasize the
importance of using available software. We start thus with a hands-on
and top-down approach to machine learning. The aim is thus to start with
relevant data or data we have produced 
and use these to introduce statistical data analysis
concepts and machine learning algorithms before we delve into the
algorithms themselves. The examples we will use in the beginning, start with simple
polynomials with random noise added. We will use the Python
software package <a href="http://scikit-learn.org/stable/" target="_blank">Scikit-learn</a> and
introduce various machine learning algorithms to make fits of
the data and predictions. We move thereafter to more interesting
cases such as the simulation of financial transactions or disease
models. These are examples where we can easily set up the data and
then use machine learning algorithms included in for example
<b>scikit-learn</b>.

<p>
These examples will serve us the purpose of getting
started. Furthermore, they allow us to catch more than two birds with
a stone. They will allow us to bring in some programming specific
topics and tools as well as showing the power of various Python (and
R) packages for machine learning and statistical data analysis. In the
lectures on linear algebra we cover in more detail various programming
features of languages like Python and C++ (and other), we will also
look into more specific linear functions which are relevant for the
various algorithms we will discuss. Here, we will mainly focus on two
specific Python packages for Machine Learning, scikit-learn and
tensorflow (see below for links etc).  Moreover, the examples we
introduce will serve as inputs to many of our discussions later, as
well as allowing you to set up models and produce your own data and
get started with programming.

<h2 id="___sec1">Software and needed installations </h2>

<p>
We will make extensive use of Python as programming language and its
myriad of available libraries.  You will find
IPython/Jupyter notebooks invaluable in your work.  You can run <b>R</b>
codes in the Jupyter/IPython notebooks, with the immediate benefit of
visualizing your data. You can also use compiled languages like C++,
Rust, Fortran etc if you prefer. The focus in these lectures will be
on Python, but we will provide many code examples for those of you who
prefer R or compiled languages. You can integrate C++ codes and R in for example
a Jupyter notebook.

<p>
If you have Python installed (we recommend Python3) and you feel
pretty familiar with installing different packages, we recommend that
you install the following Python packages via <b>pip</b> as 

<ol>
<li> pip install numpy scipy matplotlib ipython scikit-learn mglearn sympy pandas pillow</li> 
</ol>

For Python3, replace <b>pip</b> with <b>pip3</b>.

<p>
For OSX users we recommend, after having installed Xcode, to
install <b>brew</b>. Brew allows for a seamless installation of additional
software via for example 

<ol>
<li> brew install python3</li>
</ol>

For Linux users, with its variety of distributions like for example the widely popular Ubuntu distribution,
you can use <b>pip</b> as well and simply install Python as 

<ol>
<li> sudo apt-get install python3  (or python for pyhton2.7)</li>
</ol>

etc etc.

<h2 id="___sec2">Python installers </h2>

<p>
If you don't want to perform these operations separately and venture
into the hassle of exploring how to set up dependencies and paths, we
recommend two widely used distrubutions which set up all relevant
dependencies for Python, namely 

<ul>
<li> <a href="https://docs.anaconda.com/" target="_blank">Anaconda</a>,</li> 
</ul>

which is an open source
distribution of the Python and R programming languages for large-scale
data processing, predictive analytics, and scientific computing, that
aims to simplify package management and deployment. Package versions
are managed by the package management system <b>conda</b>. 

<ul>
<li> <a href="https://www.enthought.com/product/canopy/" target="_blank">Enthought canopy</a></li> 
</ul>

is a Python
distribution for scientific and analytic computing distribution and
analysis environment, available for free and under a commercial
license.

<h2 id="___sec3">Useful Python packages </h2>
Here we list several useful Python packages.

<p>
And more text to come in order to get started with Python

<h2 id="___sec4">Installing R, C++, cython or Julia </h2>

<p>
You will also find it convenient to utilize R. Although we will mainly
use Python during lectures and in various projects and exercises, we
provide a full R set of codes for the same examples. Those of you
already familiar with R should feel free to continue using R, keeping
however an eye on the parallel Python set ups. Similarly, if you are a
Python afecionado, feel free to explore R as well.  Jupyter/Ipython
notebook allows you to run <b>R</b> codes interactively in your
browser. The software library <b>R</b> is tuned to statistically analysis
and allows for an easy usage of the tools we will discuss in these
texts.

<p>
To install <b>R</b> with Jupyter notebook 
<a href="https://mpacer.org/maths/r-kernel-for-ipython-notebook" target="_blank">follow the link here</a>

<h2 id="___sec5">Installing R, C++, cython, Numba etc </h2>

<p>
For the C++ aficionados, Jupyter/IPython notebook allows you also to
install C++ and run codes written in this language interactively in
the browser. Since we will emphasize writing many of the algorithms
yourself, you can thus opt for either Python or C++ (or Fortran or other compiled languages) as programming
languages.

<p>
To add more entropy, <b>cython</b> can also be used when running your
notebooks. It means that Python with the Jupyter/IPython notebook
setup allows you to integrate widely popular softwares and tools for
scientific computing. Similarly, the 
<a href="https://numba.pydata.org/" target="_blank">Numba Python package</a> delivers increased performance
capabilities with minimal rewrites of your codes.  With its
versatility, including symbolic operations, Python offers a unique
computational environment. Your Jupyter/IPython notebook can easily be
converted into a nicely rendered <b>PDF</b> file or a Latex file for
further processing. For example, convert to latex as

<p>

<!-- code=text typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span>pycod jupyter nbconvert filename.ipynb --to latex 
</pre></div>
<p>
And to add more versatility, the Python package <a href="http://www.sympy.org/en/index.html" target="_blank">SymPy</a> is a Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS)  and is entirely written in Python.

<p>
Finally, if you wish to use the light mark-up language 
<a href="https://github.com/hplgit/doconce" target="_blank">doconce</a> you can convert a standard ascii text file into various HTML 
formats, ipython notebooks, latex files, pdf files etc with minimal edits.

<h2 id="___sec6">Numpy examples and Important Matrix and vector handling packages </h2>

<p>
There are several central software packages for linear algebra and eigenvalue problems. Several of the more
popular ones have been wrapped into ofter software packages like those from the widely used text <b>Numerical Recipes</b>. The original source codes in many of the available packages are often taken from the widely used
software package LAPACK, which follows two other popular packages
developed in the 1970s, namely EISPACK and LINPACK.  We describe them shortly here.

<ul>
  <li> LINPACK: package for linear equations and least square problems.</li>
  <li> LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems. From LAPACK's website <a href="http://www.netlib.org" target="_blank"><tt>http://www.netlib.org</tt></a> it is possible to download for free all source codes from this library. Both C/C++ and Fortran versions are available.</li>
  <li> BLAS (I, II and III): (Basic Linear Algebra Subprograms) are routines that provide standard building blocks for performing basic vector and matrix operations. Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations. Highly parallelized and efficient codes, all available for download from <a href="http://www.netlib.org" target="_blank"><tt>http://www.netlib.org</tt></a>.</li>
</ul>

When dealing with matrices and vectors a central issue is memory
handling and allocation. If our code is written in Python the way we
declare these objects and the way they are handled, interpreted and
used by say a linear algebra library, requires codes that interface
our Python program with such libraries. For Python programmers,
<b>Numpy</b> is by now the standard Python package for numerical arrays in
Python as well as the source of functions which act on these
arrays. These functions span from eigenvalue solvers to functions that
compute the mean value, variance or the covariance matrix. If you are
not familiar with how arrays are handled in say Python or compiled
languages like C++ and Fortran, the sections in this chapter may be
useful. For C++ programmer, <b>Armadillo</b> is widely used library for
linear algebra and eigenvalue problems. In addition it offers a
convenient way to handle and organize arrays. We discuss this library
as well.   Before we proceed we believe  it may be convenient to repeat some basic features of 
 matrices and vectors.

<h2 id="___sec7">Basic Matrix Features </h2>

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b>Matrix properties reminder.</b>
<p>
$$
 \mathbf{A} =
      \begin{bmatrix} a_{11} & a_{12} & a_{13} & a_{14} \\
                                 a_{21} & a_{22} & a_{23} & a_{24} \\
                                   a_{31} & a_{32} & a_{33} & a_{34} \\
                                  a_{41} & a_{42} & a_{43} & a_{44}
             \end{bmatrix}\qquad
\mathbf{I} =
      \begin{bmatrix} 1 & 0 & 0 & 0 \\
                                 0 & 1 & 0 & 0 \\
                                 0 & 0 & 1 & 0 \\
                                 0 & 0 & 0 & 1
             \end{bmatrix}
$$
</div>


<h2 id="___sec8">Basic Matrix Features </h2>
<div class="alert alert-block alert-block alert-text-normal">
<b></b>
<p>

<p>
The inverse of a matrix is defined by

$$
\mathbf{A}^{-1} \cdot \mathbf{A} = I
$$
</div>


<h2 id="___sec9">Basic Matrix Features </h2>

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b>Matrix Properties Reminder.</b>
<p>

<p>
<table border="1">
<thead>
<tr><th align="center">                Relations                 </th> <th align="center">      Name     </th> <th align="center">                              matrix elements                              </th> </tr>
</thead>
<tbody>
<tr><td align="center">   \( A = A^{T} \)                               </td> <td align="center">   symmetric          </td> <td align="center">   \( a_{ij} = a_{ji} \)                                                          </td> </tr>
<tr><td align="center">   \( A = \left (A^{T} \right )^{-1} \)          </td> <td align="center">   real orthogonal    </td> <td align="center">   \( \sum_k a_{ik} a_{jk} = \sum_k a_{ki} a_{kj} = \delta_{ij} \)                </td> </tr>
<tr><td align="center">   \( A = A^{ * } \)                             </td> <td align="center">   real matrix        </td> <td align="center">   \( a_{ij} = a_{ij}^{ * } \)                                                    </td> </tr>
<tr><td align="center">   \( A = A^{\dagger} \)                         </td> <td align="center">   hermitian          </td> <td align="center">   \( a_{ij} = a_{ji}^{ * } \)                                                    </td> </tr>
<tr><td align="center">   \( A = \left (A^{\dagger} \right )^{-1} \)    </td> <td align="center">   unitary            </td> <td align="center">   \( \sum_k a_{ik} a_{jk}^{ * } = \sum_k a_{ki}^{ * } a_{kj} = \delta_{ij} \)    </td> </tr>
</tbody>
</table>

</div>


<h2 id="___sec10">Some famous Matrices </h2>

<ul>
  <li> Diagonal if \( a_{ij}=0 \) for \( i\ne j \)</li>
  <li> Upper triangular if \( a_{ij}=0 \) for \( i > j \)</li>
  <li> Lower triangular if \( a_{ij}=0 \) for \( i < j \)</li>
  <li> Upper Hessenberg if \( a_{ij}=0 \) for \( i > j+1 \)</li>
  <li> Lower Hessenberg if \( a_{ij}=0 \) for \( i < j+1 \)</li>
  <li> Tridiagonal if \( a_{ij}=0 \) for \( |i -j| > 1 \)</li>
  <li> Lower banded with bandwidth \( p \): \( a_{ij}=0 \) for \( i > j+p \)</li>
  <li> Upper banded with bandwidth \( p \): \( a_{ij}=0 \) for \( i < j+p \)</li>
  <li> Banded, block upper triangular, block lower triangular....</li>
</ul>

<h2 id="___sec11">Basic Matrix Features </h2>

<p>
<div class="alert alert-block alert-block alert-text-normal">
<b>Some Equivalent Statements.</b>
<p>
For an \( N\times N \) matrix  \( \mathbf{A} \) the following properties are all equivalent

<ul>
  <li> If the inverse of \( \mathbf{A} \) exists, \( \mathbf{A} \) is nonsingular.</li>
  <li> The equation \( \mathbf{Ax}=0 \) implies \( \mathbf{x}=0 \).</li>
  <li> The rows of \( \mathbf{A} \) form a basis of \( R^N \).</li>
  <li> The columns of \( \mathbf{A} \) form a basis of \( R^N \).</li>
  <li> \( \mathbf{A} \) is a product of elementary matrices.</li>
  <li> \( 0 \) is not eigenvalue of \( \mathbf{A} \).</li>
</ul>
</div>


<h2 id="___sec12">Numpy and arrays </h2>
<a href="http://www.numpy.org/" target="_blank">Numpy</a> provides an easy way to handle arrays in Python. The standard way to import this library is as
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
n <span style="color: #666666">=</span> <span style="color: #666666">10</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(size<span style="color: #666666">=</span>n)
<span style="color: #008000; font-weight: bold">print</span>(x)
</pre></div>
<p>
Here we have defined a vector \( x \) with \( n=10 \) elements with its values given by the Normal distribution \( N(0,1) \).
Another alternative is to declare a vector as follows
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([<span style="color: #666666">1</span>, <span style="color: #666666">2</span>, <span style="color: #666666">3</span>])
<span style="color: #008000; font-weight: bold">print</span>(x)
</pre></div>
<p>
Here we have defined a vector with three elements, with \( x_0=1 \), \( x_1=2 \) and \( x_2=3 \). Note that both Python and C++
start numbering array elements from \( 0 \) and on. This means that a vector with \( n \) elements has a sequence of entities \( x_0, x_1, x_2, \dots, x_{n-1} \). We could also let (recommended) Numpy to compute the logarithms of a specific array as
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>log(np<span style="color: #666666">.</span>array([<span style="color: #666666">4</span>, <span style="color: #666666">7</span>, <span style="color: #666666">8</span>]))
<span style="color: #008000; font-weight: bold">print</span>(x)
</pre></div>
<p>
Here we have used Numpy's unary function \( np.log \). This function is
highly tuned to compute array elements since the code is vectorized
and does not require looping. We normaly recommend that you use the
Numpy intrinsic functions instead of the corresponding <b>log</b> function
from Python's <b>math</b> module. The looping is done explicitely by the
<b>np.log</b> function. The alternative, and slower way to compute the
logarithms of a vector would be to write

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">math</span> <span style="color: #008000; font-weight: bold">import</span> log
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([<span style="color: #666666">4</span>, <span style="color: #666666">7</span>, <span style="color: #666666">8</span>])
<span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">range</span>(<span style="color: #666666">0</span>, <span style="color: #008000">len</span>(x)):
    x[i] <span style="color: #666666">=</span> log(x[i])
<span style="color: #008000; font-weight: bold">print</span>(x)
</pre></div>
<p>
We note that our code is much longer already and we need to import the <b>log</b> function from the <b>math</b> module. 
The attentive reader will also notice that the output is \( [1, 1, 2] \). Python interprets automagically our numbers as integers (like the <b>automatic</b> keyword in C++). To change this we could define our array elements to be double precision numbers as
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>log(np<span style="color: #666666">.</span>array([<span style="color: #666666">4</span>, <span style="color: #666666">7</span>, <span style="color: #666666">8</span>], dtype <span style="color: #666666">=</span> np<span style="color: #666666">.</span>float64))
<span style="color: #008000; font-weight: bold">print</span>(x)
</pre></div>
<p>
or simply write them as double precision numbers (Python uses 64 bits as default for floating point type variables), that is
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>log(np<span style="color: #666666">.</span>array([<span style="color: #666666">4.0</span>, <span style="color: #666666">7.0</span>, <span style="color: #666666">8.0</span>])
<span style="color: #008000; font-weight: bold">print</span>(x)
</pre></div>
<p>
To check the number of bytes (remember that one byte contains eight bits for double precision variables), you can use simple use the <b>itemsize</b> functionality (the array \( x \) is actually an object which inherits the functionalities defined in Numpy) as 
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>log(np<span style="color: #666666">.</span>array([<span style="color: #666666">4.0</span>, <span style="color: #666666">7.0</span>, <span style="color: #666666">8.0</span>])
<span style="color: #008000; font-weight: bold">print</span>(x<span style="color: #666666">.</span>itemsize)
</pre></div>

<h2 id="___sec13">Matrices in Python </h2>
Having defined vectors, we are now ready to try out matrices. We can define a \( 3 \times 3  \) real matrix \( \hat{A} \)
as (recall that we user lowercase letters for vectors and uppercase letters for matrices)
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
A <span style="color: #666666">=</span> np<span style="color: #666666">.</span>log(np<span style="color: #666666">.</span>array([ [<span style="color: #666666">4.0</span>, <span style="color: #666666">7.0</span>, <span style="color: #666666">8.0</span>], [<span style="color: #666666">3.0</span>, <span style="color: #666666">10.0</span>, <span style="color: #666666">11.0</span>], [<span style="color: #666666">4.0</span>, <span style="color: #666666">5.0</span>, <span style="color: #666666">7.0</span>] ]))
<span style="color: #008000; font-weight: bold">print</span>(A)
</pre></div>
<p>
If we use the <b>shape</b> function we would get \( (3, 3) \) as output, that is verifying that our matrix is a \( 3\times 3 \) matrix. We can slice the matrix and print for example the first column (Python organized matrix elements in a row-major order, see below) as
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
A <span style="color: #666666">=</span> np<span style="color: #666666">.</span>log(np<span style="color: #666666">.</span>array([ [<span style="color: #666666">4.0</span>, <span style="color: #666666">7.0</span>, <span style="color: #666666">8.0</span>], [<span style="color: #666666">3.0</span>, <span style="color: #666666">10.0</span>, <span style="color: #666666">11.0</span>], [<span style="color: #666666">4.0</span>, <span style="color: #666666">5.0</span>, <span style="color: #666666">7.0</span>] ]))
<span style="color: #408080; font-style: italic"># print the first column, row-major order and elements start with 0</span>
<span style="color: #008000; font-weight: bold">print</span>(A[:,<span style="color: #666666">0</span>]) 
</pre></div>
<p>
We can continue this was by printing out other columns or rows. The example here prints out the second column
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
A <span style="color: #666666">=</span> np<span style="color: #666666">.</span>log(np<span style="color: #666666">.</span>array([ [<span style="color: #666666">4.0</span>, <span style="color: #666666">7.0</span>, <span style="color: #666666">8.0</span>], [<span style="color: #666666">3.0</span>, <span style="color: #666666">10.0</span>, <span style="color: #666666">11.0</span>], [<span style="color: #666666">4.0</span>, <span style="color: #666666">5.0</span>, <span style="color: #666666">7.0</span>] ]))
<span style="color: #408080; font-style: italic"># print the first column, row-major order and elements start with 0</span>
<span style="color: #008000; font-weight: bold">print</span>(A[<span style="color: #666666">1</span>,:]) 
</pre></div>
<p>
Numpy contains many other functionalities that allow us to slice, subdivide etc etc arrays. We strongly recommend that you look up the <a href="http://www.numpy.org/" target="_blank">Numpy website for more details</a>. Useful functions when defining a matrix are the <b>np.zeros</b> function which declares a matrix of a given dimension and sets all elements to zero
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
n <span style="color: #666666">=</span> <span style="color: #666666">10</span>
<span style="color: #408080; font-style: italic"># define a matrix of dimension 10 x 10 and set all elements to zero</span>
A <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros( (n, n) )
<span style="color: #008000; font-weight: bold">print</span>(A) 
</pre></div>
<p>
or initializing all elements to 
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
n <span style="color: #666666">=</span> <span style="color: #666666">10</span>
<span style="color: #408080; font-style: italic"># define a matrix of dimension 10 x 10 and set all elements to one</span>
A <span style="color: #666666">=</span> np<span style="color: #666666">.</span>ones( (n, n) )
<span style="color: #008000; font-weight: bold">print</span>(A) 
</pre></div>
<p>
or as unitarily distributed random numbers (see the material on random number generators in the statistics part)
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
n <span style="color: #666666">=</span> <span style="color: #666666">10</span>
<span style="color: #408080; font-style: italic"># define a matrix of dimension 10 x 10 and set all elements to random numbers with x \in [0, 1]</span>
A <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(n, n)
<span style="color: #008000; font-weight: bold">print</span>(A) 
</pre></div>
<p>
As we will see throughout these lectures, there are several extremely useful functionalities in Numpy.
As an example, consider the discussion of the covariance matrix. Suppose we have defined three vectors
\( \hat{x}, \hat{y}, \hat{z} \) with \( n \) elements each. The covariance matrix is defined as 
$$
\hat{\Sigma} = \begin{bmatrix} \sigma_{xx} & \sigma_{xy} & \sigma_{xz} \\
                              \sigma_{yx} & \sigma_{yy} & \sigma_{yz} \\
                              \sigma_{zx} & \sigma_{zy} & \sigma_{zz} 
             \end{bmatrix},
$$

where for example
$$
\sigma_{xy} =\frac{1}{n} \sum_{i=0}^{n-1}(x_i- \overline{x})(y_i- \overline{y}).
$$

The Numpy function <b>np.cov</b> calculates the covariance elements using the factor \( 1/(n-1) \) instead of \( 1/n \) since it assumes we do not have the exact mean values. For a more in-depth discussion of the covariance and covariance matrix and its meaning, we refer you to the lectures on statistics. 
The following simple function uses the <b>np.vstack</b> function which takes each vector of dimension \( 1\times n \) and produces a $ 3\times n$ matrix \( \hat{W} \)
$$
\hat{W} = \begin{bmatrix} x_0 & y_0 & z_0 \\
                          x_1 & y_1 & z_1 \\
                          x_2 & y_2 & z_2 \\
                          \dots & \dots & \dots \\
                          x_{n-2} & y_{n-2} & z_{n-2} \\
                          x_{n-1} & y_{n-1} & z_{n-1}
             \end{bmatrix},
$$

<p>
which in turn is converted into into the \( 3 times 3 \) covariance matrix
\( \hat{\Sigma} \) via the Numpy function <b>np.cov()</b>. In our review of
statistical functions and quantities we will discuss more about the
meaning of the covariance matrix. Here we note that we can calculate
the mean value of each set of samples \( \hat{x} \) etc using the Numpy
function <b>np.mean(x)</b>. We can also extract the eigenvalues of the
covariance matrix through the <b>np.linalg.eig()</b> function.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #408080; font-style: italic"># Importing various packages</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>

n <span style="color: #666666">=</span> <span style="color: #666666">100</span>
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(size<span style="color: #666666">=</span>n)
<span style="color: #008000; font-weight: bold">print</span>(np<span style="color: #666666">.</span>mean(x))
y <span style="color: #666666">=</span> <span style="color: #666666">4+3*</span>x<span style="color: #666666">+</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(size<span style="color: #666666">=</span>n)
<span style="color: #008000; font-weight: bold">print</span>(np<span style="color: #666666">.</span>mean(y))
z <span style="color: #666666">=</span> x<span style="color: #666666">**3+</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>normal(size<span style="color: #666666">=</span>n)
<span style="color: #008000; font-weight: bold">print</span>(np<span style="color: #666666">.</span>mean(z))
W <span style="color: #666666">=</span> np<span style="color: #666666">.</span>vstack((x, y, z))
Sigma <span style="color: #666666">=</span> np<span style="color: #666666">.</span>cov(W)
<span style="color: #008000; font-weight: bold">print</span>(Sigma)
Eigvals, Eigvecs <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linalg<span style="color: #666666">.</span>eig(Sigma)
<span style="color: #008000; font-weight: bold">print</span>(Eigvals)
</pre></div>
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">scipy</span> <span style="color: #008000; font-weight: bold">import</span> sparse
eye <span style="color: #666666">=</span> np<span style="color: #666666">.</span>eye(<span style="color: #666666">4</span>)
<span style="color: #008000; font-weight: bold">print</span>(eye)
sparse_mtx <span style="color: #666666">=</span> sparse<span style="color: #666666">.</span>csr_matrix(eye)
<span style="color: #008000; font-weight: bold">print</span>(sparse_mtx)
x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">-10</span>,<span style="color: #666666">10</span>,<span style="color: #666666">100</span>)
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>sin(x)
plt<span style="color: #666666">.</span>plot(x,y,marker<span style="color: #666666">=</span><span style="color: #BA2121">&#39;x&#39;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
Another useful Python package is
<a href="https://pandas.pydata.org/" target="_blank">pandas</a>, which is an open source library
providing high-performance, easy-to-use data structures and data
analysis tools for Python. The following simple example shows how we can, in an easy way make tables of our data. Here we define a data set which includes names, city of residence and age, and displays the data in an easy to read way. We will see repeated use of <b>pandas</b>, in particular in connection with classification of data.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">IPython.display</span> <span style="color: #008000; font-weight: bold">import</span> display
data <span style="color: #666666">=</span> {<span style="color: #BA2121">&#39;Name&#39;</span>: [<span style="color: #BA2121">&quot;John&quot;</span>, <span style="color: #BA2121">&quot;Anna&quot;</span>, <span style="color: #BA2121">&quot;Peter&quot;</span>, <span style="color: #BA2121">&quot;Linda&quot;</span>], <span style="color: #BA2121">&#39;Location&#39;</span>: [<span style="color: #BA2121">&quot;Nairobi&quot;</span>, <span style="color: #BA2121">&quot;Napoli&quot;</span>, <span style="color: #BA2121">&quot;London&quot;</span>, <span style="color: #BA2121">&quot;Buenos Aires&quot;</span>], <span style="color: #BA2121">&#39;Age&#39;</span>:[<span style="color: #666666">51</span>, <span style="color: #666666">21</span>, <span style="color: #666666">34</span>, <span style="color: #666666">45</span>]}
data_pandas <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>DataFrame(data)
display(data_pandas)
</pre></div>
<p>
Here are other examples where we use the <b>DataFrame</b> functionality to handle arrays
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>                                                                               
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">IPython.display</span> <span style="color: #008000; font-weight: bold">import</span> display
np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>seed(<span style="color: #666666">100</span>)                                                                          
<span style="color: #408080; font-style: italic"># setting up a 9 x 4 matrix</span>
a <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">9</span>,<span style="color: #666666">4</span>)
df <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>DataFrame(a)   
display(df)
<span style="color: #008000; font-weight: bold">print</span>(df<span style="color: #666666">.</span>mean())
<span style="color: #008000; font-weight: bold">print</span>(df<span style="color: #666666">.</span>std())
display(df<span style="color: #666666">**2</span>)
</pre></div>

<h2 id="___sec14">Reading Data </h2>

<p>
In order to study various Machine Learning algorithms, we need to
access data. Acccessing data is an essential step in all machine
learning algorithms. In particular, setting up the so-called <b>design
matrix</b> (to be defined below) is often the first element we need in
order to perform our calculations. To set up the design matrix means
reading (and later, when the calculations are done, writing data) data
in various formats, The formats span from reading files from disk,
loading data from databases and interacting with online sources
like web application programming interfaces (APIs).

<p>
In handling various input formats, we will mainly stay with <b>pandas</b>,
a Python package which allows us in a seamless and painless way to
deal with a multitude of formats, from standars <b>csv</b> (comma separated
values) files, via <b>excel</b>, <b>html</b> to <b>hdf5</b> formats.  With <b>pandas</b>
and the <b>DataFrame</b> functionality we are able to convert text data
into the calculational formats we need for a specific algorithm.

<p>
We are going to start with a classic from nuclear physics, namely all
available data on binding energies. We will then show some of the
strength of packages like <b>scikitlearn</b> in fitting binding energies to
specific functions using linear regression.

<p>
But let us start with reading and organizing our data. This will also allow us to present <b>matplotlib</b>, 
a powerful Python package for plotting. The OECD and its <a href="http://www.oecd-nea.org/dbdata/data/" target="_blank">Nuclear Energy Agency</a>
have posted the full list of masses from the compilation of <a href="https://www.sciencedirect.com/science/article/pii/S0375947403018086?via%3Dihub" target="_blank">Audi and Wapstra from 2003</a>. Later versions are also available.

<p>
After having downloaded this file to our own computer, we are now ready to read the file and start structuring our data.

<p>
<!-- to do, make a general read and write files to folders. prepare these folders -->
<!-- Look at different ways of handling data, make also csv file -->
<!-- Have additional plot of separation energies etc -->

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #408080; font-style: italic"># Common imports</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">os</span>

<span style="color: #408080; font-style: italic"># Where to save the figures and data files</span>
PROJECT_ROOT_DIR <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results&quot;</span>
FIGURE_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Results/FigureFiles&quot;</span>
DATA_ID <span style="color: #666666">=</span> <span style="color: #BA2121">&quot;DataFiles/&quot;</span>

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(PROJECT_ROOT_DIR):
    os<span style="color: #666666">.</span>mkdir(PROJECT_ROOT_DIR)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(FIGURE_ID):
    os<span style="color: #666666">.</span>makedirs(FIGURE_ID)

<span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>exists(DATA_ID):
    os<span style="color: #666666">.</span>makedirs(DATA_ID)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">image_path</span>(fig_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(FIGURE_ID, fig_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">data_path</span>(dat_id):
    <span style="color: #008000; font-weight: bold">return</span> os<span style="color: #666666">.</span>path<span style="color: #666666">.</span>join(DATA_ID, dat_id)

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">save_fig</span>(fig_id):
    plt<span style="color: #666666">.</span>savefig(image_path(fig_id) <span style="color: #666666">+</span> <span style="color: #BA2121">&quot;.png&quot;</span>, format<span style="color: #666666">=</span><span style="color: #BA2121">&#39;png&#39;</span>)

infile <span style="color: #666666">=</span> <span style="color: #008000">open</span>(data_path(<span style="color: #BA2121">&quot;MassEval2016.dat&quot;</span>),<span style="color: #BA2121">&#39;r&#39;</span>)

<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">SEMF</span>(Z, N):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;Calculate the average binding energy per nucleon for nucleus Z, N.</span>

<span style="color: #BA2121; font-style: italic">    Calculate the average nuclear binding energy per nucleon for a nucleus</span>
<span style="color: #BA2121; font-style: italic">    with Z protons and N neutrons, using the semi-empirical mass formula and</span>
<span style="color: #BA2121; font-style: italic">    parameters of J. W. Rohlf, &quot;Modern Physics from alpha to Z0&quot;, Wiley (1994).</span>

<span style="color: #BA2121; font-style: italic">    Z and N can be NumPy arrays or scalar values.</span>

<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>

    <span style="color: #408080; font-style: italic"># The parameterization of the SEMF to use.</span>
    aV, aS, aC, aA, delta <span style="color: #666666">=</span> <span style="color: #666666">15.75</span>, <span style="color: #666666">17.8</span>, <span style="color: #666666">0.711</span>, <span style="color: #666666">23.7</span>, <span style="color: #666666">11.18</span>

    <span style="color: #408080; font-style: italic"># Covert Z and N to NumPy arrays if they aren&#39;t already</span>
    Z, N <span style="color: #666666">=</span> np<span style="color: #666666">.</span>atleast_1d(Z), np<span style="color: #666666">.</span>atleast_1d(N)
    <span style="color: #408080; font-style: italic"># Total number of nucleons</span>
    A <span style="color: #666666">=</span> Z <span style="color: #666666">+</span> N

    <span style="color: #408080; font-style: italic"># The pairing term is -delta for Z and N both odd, +delta for Z and N both</span>
    <span style="color: #408080; font-style: italic"># even, and 0 otherwise. Create an array of the sign of this term so that</span>
    <span style="color: #408080; font-style: italic"># we can vectorize the calculation across the arrays Z and N.</span>
    sgn <span style="color: #666666">=</span> np<span style="color: #666666">.</span>zeros(Z<span style="color: #666666">.</span>shape)
    sgn[(Z<span style="color: #666666">%2</span>) <span style="color: #666666">&amp;</span> (N<span style="color: #666666">%2</span>)] <span style="color: #666666">=</span> <span style="color: #666666">-1</span>
    sgn[<span style="color: #666666">~</span>(Z<span style="color: #666666">%2</span>) <span style="color: #666666">&amp;</span> <span style="color: #666666">~</span>(N<span style="color: #666666">%2</span>)] <span style="color: #666666">=</span> <span style="color: #666666">+1</span>

    <span style="color: #408080; font-style: italic"># The SEMF for the average binding energy per nucleon.</span>
    E <span style="color: #666666">=</span> (aV <span style="color: #666666">-</span> aS <span style="color: #666666">/</span> A<span style="color: #666666">**</span>(<span style="color: #666666">1/3</span>) <span style="color: #666666">-</span> aC <span style="color: #666666">*</span> Z<span style="color: #666666">**2</span> <span style="color: #666666">/</span> A<span style="color: #666666">**</span>(<span style="color: #666666">4/3</span>) <span style="color: #666666">-</span>
         aA <span style="color: #666666">*</span> (A<span style="color: #666666">-2*</span>Z)<span style="color: #666666">**2/</span>A<span style="color: #666666">**2</span> <span style="color: #666666">+</span> sgn <span style="color: #666666">*</span> delta<span style="color: #666666">/</span>A<span style="color: #666666">**</span>(<span style="color: #666666">3/2</span>))

    <span style="color: #408080; font-style: italic"># Return E as a scalar or array as appropriate to the input Z.</span>
    <span style="color: #008000; font-weight: bold">if</span> Z<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>] <span style="color: #666666">==</span> <span style="color: #666666">1</span>:
        <span style="color: #008000; font-weight: bold">return</span> <span style="color: #008000">float</span>(E)
    <span style="color: #008000; font-weight: bold">return</span> E

<span style="color: #408080; font-style: italic"># Read the experimental data into a Pandas DataFrame.</span>
df <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>read_fwf(infile, usecols<span style="color: #666666">=</span>(<span style="color: #666666">2</span>,<span style="color: #666666">3</span>,<span style="color: #666666">4</span>,<span style="color: #666666">11</span>),
              names<span style="color: #666666">=</span>(<span style="color: #BA2121">&#39;N&#39;</span>, <span style="color: #BA2121">&#39;Z&#39;</span>, <span style="color: #BA2121">&#39;A&#39;</span>, <span style="color: #BA2121">&#39;avEbind&#39;</span>),
              widths<span style="color: #666666">=</span>(<span style="color: #666666">1</span>,<span style="color: #666666">3</span>,<span style="color: #666666">5</span>,<span style="color: #666666">5</span>,<span style="color: #666666">5</span>,<span style="color: #666666">1</span>,<span style="color: #666666">3</span>,<span style="color: #666666">4</span>,<span style="color: #666666">1</span>,<span style="color: #666666">13</span>,<span style="color: #666666">11</span>,<span style="color: #666666">11</span>,<span style="color: #666666">9</span>,<span style="color: #666666">1</span>,<span style="color: #666666">2</span>,<span style="color: #666666">11</span>,<span style="color: #666666">9</span>,<span style="color: #666666">1</span>,<span style="color: #666666">3</span>,<span style="color: #666666">1</span>,<span style="color: #666666">12</span>,<span style="color: #666666">11</span>,<span style="color: #666666">1</span>),
              header<span style="color: #666666">=39</span>,
              index_col<span style="color: #666666">=</span><span style="color: #008000">False</span>)

<span style="color: #408080; font-style: italic"># Extrapolated values are indicated by &#39;#&#39; in place of the decimal place, so</span>
<span style="color: #408080; font-style: italic"># the avEbind column won&#39;t be numeric. Coerce to float and drop these entries.</span>
df[<span style="color: #BA2121">&#39;avEbind&#39;</span>] <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>to_numeric(df[<span style="color: #BA2121">&#39;avEbind&#39;</span>], errors<span style="color: #666666">=</span><span style="color: #BA2121">&#39;coerce&#39;</span>)
df <span style="color: #666666">=</span> df<span style="color: #666666">.</span>dropna()
<span style="color: #408080; font-style: italic"># Also convert from keV to MeV.</span>
df[<span style="color: #BA2121">&#39;avEbind&#39;</span>] <span style="color: #666666">/=</span> <span style="color: #666666">1000</span>

<span style="color: #408080; font-style: italic"># Group the DataFrame by nucleon number, A.</span>
gdf <span style="color: #666666">=</span> df<span style="color: #666666">.</span>groupby(<span style="color: #BA2121">&#39;A&#39;</span>)
<span style="color: #408080; font-style: italic"># Find the rows of the grouped DataFrame with the maximum binding energy.</span>
maxavEbind <span style="color: #666666">=</span> gdf<span style="color: #666666">.</span>apply(<span style="color: #008000; font-weight: bold">lambda</span> t: t[t<span style="color: #666666">.</span>avEbind<span style="color: #666666">==</span>t<span style="color: #666666">.</span>avEbind<span style="color: #666666">.</span>max()])

<span style="color: #408080; font-style: italic"># Add a column of estimated binding energies calculated using the SEMF.</span>
maxavEbind[<span style="color: #BA2121">&#39;Eapprox&#39;</span>] <span style="color: #666666">=</span> SEMF(maxavEbind[<span style="color: #BA2121">&#39;Z&#39;</span>], maxavEbind[<span style="color: #BA2121">&#39;N&#39;</span>])

<span style="color: #408080; font-style: italic"># Generate a plot comparing the experimental with the SEMF values.</span>
fig, ax <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots()
ax<span style="color: #666666">.</span>plot(maxavEbind[<span style="color: #BA2121">&#39;A&#39;</span>], maxavEbind[<span style="color: #BA2121">&#39;avEbind&#39;</span>], alpha<span style="color: #666666">=0.7</span>, lw<span style="color: #666666">=2</span>,
            label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Ame2003&#39;</span>)
ax<span style="color: #666666">.</span>plot(maxavEbind[<span style="color: #BA2121">&#39;A&#39;</span>], maxavEbind[<span style="color: #BA2121">&#39;Eapprox&#39;</span>], alpha<span style="color: #666666">=0.7</span>, lw<span style="color: #666666">=2</span>, c<span style="color: #666666">=</span><span style="color: #BA2121">&#39;m&#39;</span>,
            label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;SEMF&#39;</span>)
ax<span style="color: #666666">.</span>set_xlabel(<span style="color: #BA2121">r&#39;$A = N + Z$&#39;</span>)
ax<span style="color: #666666">.</span>set_ylabel(<span style="color: #BA2121">r&#39;$E_\mathrm{bind}\,/\mathrm{MeV}$&#39;</span>)
ax<span style="color: #666666">.</span>legend()
<span style="color: #408080; font-style: italic"># We don&#39;t expect the SEMF to work very well for light nuclei with small</span>
<span style="color: #408080; font-style: italic"># average binding energies, so display only data relevant to avEbind &gt; 7 MeV.</span>
ax<span style="color: #666666">.</span>set_ylim(<span style="color: #666666">7</span>,<span style="color: #666666">9</span>)
save_fig(<span style="color: #BA2121">&quot;Masses2016&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>

<h2 id="___sec15">Simple linear regression model using <b>scikit-learn</b> </h2>

<p>
We start with perhaps our simplest possible example, using <b>scikit-learn</b> to perform linear regression analysis on a data set produced by us. 
What follows is a simple Python code where we have defined  function \( y \) in terms of the variable \( x \). Both are defined as vectors of dimension \( 1\times 100 \). The entries to the vector \( \hat{x} \)  are given by random numbers generated with a uniform distribution with entries \( x_i \in [0,1] \) (more about probability distribution functions later). These values are then used to define a function \( y(x) \) (tabulated again as a vector) with a linear dependence on \( x \) plus a random noise added via the normal distribution.

<p>
The Numpy functions are imported used the <b>import numpy as np</b>
statement and the random number generator for the uniform distribution
is called using the function <b>np.random.rand()</b>, where we specificy
that we want \( 100 \) random variables.  Using Numpy we define
automatically an array with the specified number of elements, \( 100 \) in
our case.  With the Numpy function <b>randn()</b> we can compute random
numbers with the normal distribution (mean value \( \mu \) equal to zero and
variance \( \sigma^2 \) set to one) and produce the values of \( y \) assuming a linear
dependence as function of \( x \)

$$
y = 2x+N(0,1),
$$

<p>
where \( N(0,1) \) represents random numbers generated by the normal
distribution.  From <b>scikit-learn</b> we import then the
<b>LinearRegression</b> functionality and make a prediction \( \tilde{y} =
\alpha + \beta x \) using the function <b>fit(x,y)</b>. We call the set of
data \( (\hat{x},\hat{y}) \) for our training data. The Python package
<b>scikit-learn</b> has also a functionality which extracts the above
fitting parameters \( \alpha \) and \( \beta \) (see below). Later we will
distinguish between training data and test data.

<p>
For plotting we use the Python package
<a href="https://matplotlib.org/" target="_blank">matplotlib</a> which produces publication
quality figures. Feel free to explore the extensive
<a href="https://matplotlib.org/gallery/index.html" target="_blank">gallery</a> of examples. In
this example we plot our original values of \( x \) and \( y \) as well as the
prediction <b>ypredict</b> (\( \tilde{y} \)), which attempts at fitting our
data with a straight line.

<p>
The Python code follows here.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #408080; font-style: italic"># Importing various packages</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">2*</span>x<span style="color: #666666">+</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
linreg <span style="color: #666666">=</span> LinearRegression()
linreg<span style="color: #666666">.</span>fit(x,y)
xnew <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([[<span style="color: #666666">0</span>],[<span style="color: #666666">1</span>]])
ypredict <span style="color: #666666">=</span> linreg<span style="color: #666666">.</span>predict(xnew)

plt<span style="color: #666666">.</span>plot(xnew, ypredict, <span style="color: #BA2121">&quot;r-&quot;</span>)
plt<span style="color: #666666">.</span>plot(x, y ,<span style="color: #BA2121">&#39;ro&#39;</span>)
plt<span style="color: #666666">.</span>axis([<span style="color: #666666">0</span>,<span style="color: #666666">1.0</span>,<span style="color: #666666">0</span>, <span style="color: #666666">5.0</span>])
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">r&#39;$x$&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">r&#39;$y$&#39;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">r&#39;Simple Linear Regression&#39;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
This example serves several aims. It allows us to demonstrate several
aspects of data analysis and later machine learning algorithms. The
immediate visualization shows that our linear fit is not
impressive. It goes through the data points, but there are many
outliers which are not reproduced by our linear regression.  We could
now play around with this small program and change for example the
factor in front of \( x \) and the normal distribution.  Try to change the
function \( y \) to

$$
y = 10x+0.01 \times N(0,1),
$$

<p>
where \( x \) is defined as before.  Does the fit look better? Indeed, by
reducing the role of the normal distribution we see immediately that
our linear prediction seemingly reproduces better the training
set. However, this testing 'by the eye' is obviouly not satisfactory in the
long run. Here we have only defined the training data and our model, and 
have not discussed a more rigorous approach to the <b>cost</b> function.

<p>
We need more rigorous criteria in defining whether we have succeeded or
not in modeling our training data.  You will be surprised to see that
many scientists seldomly venture beyond this 'by the eye' approach. A
standard approach for the <em>cost</em> function is the so-called \( \chi^2 \)
function 

$$ \chi^2 = \frac{1}{n}
\sum_{i=0}^{n-1}\frac{(y_i-\tilde{y}_i)^2}{\sigma_i^2}, 
$$

<p>
where \( \sigma_i^2 \) is the variance (to be defined later) of the entry
\( y_i \).  We may not know the explicit value of \( \sigma_i^2 \), it serves
however the aim of scaling the equations and make the cost function
dimensionless.

<p>
Minimizing the cost function is a central aspect of
our discussions to come. Finding its minima as function of the model
parameters (\( \alpha \) and \( \beta \) in our case) will be a recurring
theme in these series of lectures. Essentially all machine learning
algorithms we will discuss center around the minimization of the
chosen cost function. This depends in turn on our specific
model for describing the data, a typical situation in supervised
learning. Automatizing the search for the minima of the cost function is a
central ingredient in all algorithms. Typical methods which are
employed are various variants of <b>gradient</b> methods. These will be
discussed in more detail later. Again, you'll be surprised to hear that
many practitioners minimize the above function ''by the eye', popularly dubbed as 
'chi by the eye'. That is, change a parameter and see (visually and numerically) that 
the  \( \chi^2 \) function becomes smaller.

<p>
There are many ways to define the cost function. A simpler approach is to look at the relative difference between the training data and the predicted data, that is we define 
the relative error as

$$
\epsilon_{\mathrm{relative}}= \frac{\vert \hat{y} -\hat{\tilde{y}}\vert}{\vert \hat{y}\vert}.
$$

We can modify easily the above Python code and plot the relative instead
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">5*</span>x<span style="color: #666666">+0.01*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
linreg <span style="color: #666666">=</span> LinearRegression()
linreg<span style="color: #666666">.</span>fit(x,y)
ypredict <span style="color: #666666">=</span> linreg<span style="color: #666666">.</span>predict(x)

plt<span style="color: #666666">.</span>plot(x, np<span style="color: #666666">.</span>abs(ypredict<span style="color: #666666">-</span>y)<span style="color: #666666">/</span><span style="color: #008000">abs</span>(y), <span style="color: #BA2121">&quot;ro&quot;</span>)
plt<span style="color: #666666">.</span>axis([<span style="color: #666666">0</span>,<span style="color: #666666">1.0</span>,<span style="color: #666666">0.0</span>, <span style="color: #666666">0.5</span>])
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">r&#39;$x$&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">r&#39;$\epsilon_{\mathrm{relative}}$&#39;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">r&#39;Relative error&#39;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
Depending on the parameter in front of the normal distribution, we may
have a small or larger relative error. Try to play around with
different training data sets and study (graphically) the value of the
relative error.

<p>
As mentioned above, <b>scikit-learn</b> has an impressive functionality.
We can for example extract the values of \( \alpha \) and \( \beta \) and
their error estimates, or the variance and standard deviation and many
other properties from the statistical data analysis.

<p>
Here we show an
example of the functionality of scikit-learn.
<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span> 
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span> 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression 
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.metrics</span> <span style="color: #008000; font-weight: bold">import</span> mean_squared_error, r2_score, mean_squared_log_error, mean_absolute_error

x <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
y <span style="color: #666666">=</span> <span style="color: #666666">2.0+</span> <span style="color: #666666">5*</span>x<span style="color: #666666">+0.5*</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randn(<span style="color: #666666">100</span>,<span style="color: #666666">1</span>)
linreg <span style="color: #666666">=</span> LinearRegression()
linreg<span style="color: #666666">.</span>fit(x,y)
ypredict <span style="color: #666666">=</span> linreg<span style="color: #666666">.</span>predict(x)
<span style="color: #008000; font-weight: bold">print</span>(<span style="color: #BA2121">&#39;The intercept alpha: </span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">&#39;</span>, linreg<span style="color: #666666">.</span>intercept_)
<span style="color: #008000; font-weight: bold">print</span>(<span style="color: #BA2121">&#39;Coefficient beta : </span><span style="color: #BB6622; font-weight: bold">\n</span><span style="color: #BA2121">&#39;</span>, linreg<span style="color: #666666">.</span>coef_)
<span style="color: #408080; font-style: italic"># The mean squared error                               </span>
<span style="color: #008000; font-weight: bold">print</span>(<span style="color: #BA2121">&quot;Mean squared error: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&quot;</span> <span style="color: #666666">%</span> mean_squared_error(y, ypredict))
<span style="color: #408080; font-style: italic"># Explained variance score: 1 is perfect prediction                                 </span>
<span style="color: #008000; font-weight: bold">print</span>(<span style="color: #BA2121">&#39;Variance score: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&#39;</span> <span style="color: #666666">%</span> r2_score(y, ypredict))
<span style="color: #408080; font-style: italic"># Mean squared log error                                                        </span>
<span style="color: #008000; font-weight: bold">print</span>(<span style="color: #BA2121">&#39;Mean squared log error: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&#39;</span> <span style="color: #666666">%</span> mean_squared_log_error(y, ypredict) )
<span style="color: #408080; font-style: italic"># Mean absolute error                                                           </span>
<span style="color: #008000; font-weight: bold">print</span>(<span style="color: #BA2121">&#39;Mean absolute error: </span><span style="color: #BB6688; font-weight: bold">%.2f</span><span style="color: #BA2121">&#39;</span> <span style="color: #666666">%</span> mean_absolute_error(y, ypredict))
plt<span style="color: #666666">.</span>plot(x, ypredict, <span style="color: #BA2121">&quot;r-&quot;</span>)
plt<span style="color: #666666">.</span>plot(x, y ,<span style="color: #BA2121">&#39;ro&#39;</span>)
plt<span style="color: #666666">.</span>axis([<span style="color: #666666">0.0</span>,<span style="color: #666666">1.0</span>,<span style="color: #666666">1.5</span>, <span style="color: #666666">7.0</span>])
plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">r&#39;$x$&#39;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">r&#39;$y$&#39;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">r&#39;Linear Regression fit &#39;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>
The function <b>coef</b> gives us the parameter \( \beta \) of our fit while <b>intercept</b> yields 
\( \alpha \). Depending on the constant in front of the normal distribution, we get values near or far from \( alpha =2 \) and \( \beta =5 \). Try to play around with different parameters in front of the normal distribution. The function <b>meansquarederror</b> gives us the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error or loss defined as
$$ MSE(\hat{y},\hat{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2, 
$$

<p>
The smaller the value, the better the fit. Ideally we would like to
have an MSE equal zero.  The attentive reader has probably recognized
this function as being similar to the \( \chi^2 \) function defined above.

<p>
The <b>r2score</b> function computes \( R^2 \), the coefficient of
determination. It provides a measure of how well future samples are
likely to be predicted by the model. Best possible score is 1.0 and it
can be negative (because the model can be arbitrarily worse). A
constant model that always predicts the expected value of \( \hat{y} \),
disregarding the input features, would get a \( R^2 \) score of \( 0.0 \).

<p>
If \( \tilde{\hat{y}}_i \) is the predicted value of the \( i-th \) sample and \( y_i \) is the corresponding true value, then the score \( R^2 \) is defined as
$$
R^2(\hat{y}, \tilde{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
$$

where we have defined the mean value  of \( \hat{y} \) as
$$
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
$$

Another quantity will meet again in our discussions of regression analysis is 
 mean absolute error (MAE), a risk metric corresponding to the expected value of the absolute error loss or what we call the \( l1 \)-norm loss. In our discussion above we presented the relative error.
The MAE is defined as follows
$$
\text{MAE}(\hat{y}, \hat{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n-1} \left| y_i - \tilde{y}_i \right|.
$$

Finally we present the 
squared logarithmic (quadratic) error
$$
\text{MSLE}(\hat{y}, \hat{\tilde{y}}) = \frac{1}{n} \sum_{i=0}^{n - 1} (\log_e (1 + y_i) - \log_e (1 + \tilde{y}_i) )^2,
$$

<p>
where \( \log_e (x) \) stands for the natural logarithm of \( x \). This error
estimate is best to use when targets having exponential growth, such
as population counts, average sales of a commodity over a span of
years etc.

<p>
We will discuss in more
detail these and other functions in the various lectures.  We conclude this part with another example. Instead of 
a linear \( x \)-dependence we study now a cubic polynomial and use the polynomial regression analysis tools of scikit-learn.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">random</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> Ridge
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.pipeline</span> <span style="color: #008000; font-weight: bold">import</span> make_pipeline
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression

x<span style="color: #666666">=</span>np<span style="color: #666666">.</span>linspace(<span style="color: #666666">0.02</span>,<span style="color: #666666">0.98</span>,<span style="color: #666666">200</span>)
noise <span style="color: #666666">=</span> np<span style="color: #666666">.</span>asarray(random<span style="color: #666666">.</span>sample((<span style="color: #008000">range</span>(<span style="color: #666666">200</span>)),<span style="color: #666666">200</span>))
y<span style="color: #666666">=</span>x<span style="color: #666666">**3*</span>noise
yn<span style="color: #666666">=</span>x<span style="color: #666666">**3*100</span>
poly3 <span style="color: #666666">=</span> PolynomialFeatures(degree<span style="color: #666666">=3</span>)
X <span style="color: #666666">=</span> poly3<span style="color: #666666">.</span>fit_transform(x[:,np<span style="color: #666666">.</span>newaxis])
clf3 <span style="color: #666666">=</span> LinearRegression()
clf3<span style="color: #666666">.</span>fit(X,y)

Xplot<span style="color: #666666">=</span>poly3<span style="color: #666666">.</span>fit_transform(x[:,np<span style="color: #666666">.</span>newaxis])
poly3_plot<span style="color: #666666">=</span>plt<span style="color: #666666">.</span>plot(x, clf3<span style="color: #666666">.</span>predict(Xplot), label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Cubic Fit&#39;</span>)
plt<span style="color: #666666">.</span>plot(x,yn, color<span style="color: #666666">=</span><span style="color: #BA2121">&#39;red&#39;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;True Cubic&quot;</span>)
plt<span style="color: #666666">.</span>scatter(x, y, label<span style="color: #666666">=</span><span style="color: #BA2121">&#39;Data&#39;</span>, color<span style="color: #666666">=</span><span style="color: #BA2121">&#39;orange&#39;</span>, s<span style="color: #666666">=15</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">error</span>(a):
    <span style="color: #008000; font-weight: bold">for</span> i <span style="color: #AA22FF; font-weight: bold">in</span> y:
        err<span style="color: #666666">=</span>(y<span style="color: #666666">-</span>yn)<span style="color: #666666">/</span>yn
    <span style="color: #008000; font-weight: bold">return</span> <span style="color: #008000">abs</span>(np<span style="color: #666666">.</span>sum(err))<span style="color: #666666">/</span><span style="color: #008000">len</span>(err)

<span style="color: #008000; font-weight: bold">print</span> (error(y))
</pre></div>
<p>
Similarly, using <b>R</b>, we can perform similar studies.

<h3 id="___sec16">Random walk model </h3>

We end with a  teaser on how to fit a random walk with up to tenth-order polynomial as well as using decision trees as algorithm.

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.preprocessing</span> <span style="color: #008000; font-weight: bold">import</span> PolynomialFeatures
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression

steps<span style="color: #666666">=250</span>

distance<span style="color: #666666">=0</span>
x<span style="color: #666666">=0</span>
distance_list<span style="color: #666666">=</span>[]
steps_list<span style="color: #666666">=</span>[]
<span style="color: #008000; font-weight: bold">while</span> x<span style="color: #666666">&lt;</span>steps:
    distance<span style="color: #666666">+=</span>np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>randint(<span style="color: #666666">-1</span>,<span style="color: #666666">2</span>)
    distance_list<span style="color: #666666">.</span>append(distance)
    x<span style="color: #666666">+=1</span>
    steps_list<span style="color: #666666">.</span>append(x)
plt<span style="color: #666666">.</span>plot(steps_list,distance_list, color<span style="color: #666666">=</span><span style="color: #BA2121">&#39;green&#39;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;Random Walk Data&quot;</span>)

steps_list<span style="color: #666666">=</span>np<span style="color: #666666">.</span>asarray(steps_list)
distance_list<span style="color: #666666">=</span>np<span style="color: #666666">.</span>asarray(distance_list)

X<span style="color: #666666">=</span>steps_list[:,np<span style="color: #666666">.</span>newaxis]

<span style="color: #408080; font-style: italic">#Polynomial fits</span>

<span style="color: #408080; font-style: italic">#Degree 2</span>
poly_features<span style="color: #666666">=</span>PolynomialFeatures(degree<span style="color: #666666">=2</span>, include_bias<span style="color: #666666">=</span><span style="color: #008000">False</span>)
X_poly<span style="color: #666666">=</span>poly_features<span style="color: #666666">.</span>fit_transform(X)

lin_reg<span style="color: #666666">=</span>LinearRegression()
poly_fit<span style="color: #666666">=</span>lin_reg<span style="color: #666666">.</span>fit(X_poly,distance_list)
b<span style="color: #666666">=</span>lin_reg<span style="color: #666666">.</span>coef_
c<span style="color: #666666">=</span>lin_reg<span style="color: #666666">.</span>intercept_
<span style="color: #008000; font-weight: bold">print</span> (<span style="color: #BA2121">&quot;2nd degree coefficients:&quot;</span>)
<span style="color: #008000; font-weight: bold">print</span> (<span style="color: #BA2121">&quot;zero power: &quot;</span>,c)
<span style="color: #008000; font-weight: bold">print</span> (<span style="color: #BA2121">&quot;first power: &quot;</span>, b[<span style="color: #666666">0</span>])
<span style="color: #008000; font-weight: bold">print</span> (<span style="color: #BA2121">&quot;second power: &quot;</span>,b[<span style="color: #666666">1</span>])

z <span style="color: #666666">=</span> np<span style="color: #666666">.</span>arange(<span style="color: #666666">0</span>, steps, <span style="color: #666666">.01</span>)
z_mod<span style="color: #666666">=</span>b[<span style="color: #666666">1</span>]<span style="color: #666666">*</span>z<span style="color: #666666">**2+</span>b[<span style="color: #666666">0</span>]<span style="color: #666666">*</span>z<span style="color: #666666">+</span>c

fit_mod<span style="color: #666666">=</span>b[<span style="color: #666666">1</span>]<span style="color: #666666">*</span>X<span style="color: #666666">**2+</span>b[<span style="color: #666666">0</span>]<span style="color: #666666">*</span>X<span style="color: #666666">+</span>c
plt<span style="color: #666666">.</span>plot(z, z_mod, color<span style="color: #666666">=</span><span style="color: #BA2121">&#39;r&#39;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;2nd Degree Fit&quot;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Polynomial Regression&quot;</span>)

plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;Steps&quot;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;Distance&quot;</span>)

<span style="color: #408080; font-style: italic">#Degree 10</span>
poly_features10<span style="color: #666666">=</span>PolynomialFeatures(degree<span style="color: #666666">=10</span>, include_bias<span style="color: #666666">=</span><span style="color: #008000">False</span>)
X_poly10<span style="color: #666666">=</span>poly_features10<span style="color: #666666">.</span>fit_transform(X)

poly_fit10<span style="color: #666666">=</span>lin_reg<span style="color: #666666">.</span>fit(X_poly10,distance_list)

y_plot<span style="color: #666666">=</span>poly_fit10<span style="color: #666666">.</span>predict(X_poly10)
plt<span style="color: #666666">.</span>plot(X, y_plot, color<span style="color: #666666">=</span><span style="color: #BA2121">&#39;black&#39;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;10th Degree Fit&quot;</span>)

plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()


<span style="color: #408080; font-style: italic">#Decision Tree Regression</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeRegressor
regr_1<span style="color: #666666">=</span>DecisionTreeRegressor(max_depth<span style="color: #666666">=2</span>)
regr_2<span style="color: #666666">=</span>DecisionTreeRegressor(max_depth<span style="color: #666666">=5</span>)
regr_3<span style="color: #666666">=</span>DecisionTreeRegressor(max_depth<span style="color: #666666">=7</span>)
regr_1<span style="color: #666666">.</span>fit(X, distance_list)
regr_2<span style="color: #666666">.</span>fit(X, distance_list)
regr_3<span style="color: #666666">.</span>fit(X, distance_list)

X_test <span style="color: #666666">=</span> np<span style="color: #666666">.</span>arange(<span style="color: #666666">0.0</span>, steps, <span style="color: #666666">0.01</span>)[:, np<span style="color: #666666">.</span>newaxis]
y_1 <span style="color: #666666">=</span> regr_1<span style="color: #666666">.</span>predict(X_test)
y_2 <span style="color: #666666">=</span> regr_2<span style="color: #666666">.</span>predict(X_test)
y_3<span style="color: #666666">=</span>regr_3<span style="color: #666666">.</span>predict(X_test)

<span style="color: #408080; font-style: italic"># Plot the results</span>
plt<span style="color: #666666">.</span>figure()
plt<span style="color: #666666">.</span>scatter(X, distance_list, s<span style="color: #666666">=2.5</span>, c<span style="color: #666666">=</span><span style="color: #BA2121">&quot;black&quot;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;data&quot;</span>)
plt<span style="color: #666666">.</span>plot(X_test, y_1, color<span style="color: #666666">=</span><span style="color: #BA2121">&quot;red&quot;</span>,
         label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;max_depth=2&quot;</span>, linewidth<span style="color: #666666">=2</span>)
plt<span style="color: #666666">.</span>plot(X_test, y_2, color<span style="color: #666666">=</span><span style="color: #BA2121">&quot;green&quot;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;max_depth=5&quot;</span>, linewidth<span style="color: #666666">=2</span>)
plt<span style="color: #666666">.</span>plot(X_test, y_3, color<span style="color: #666666">=</span><span style="color: #BA2121">&quot;m&quot;</span>, label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;max_depth=7&quot;</span>, linewidth<span style="color: #666666">=2</span>)

plt<span style="color: #666666">.</span>xlabel(<span style="color: #BA2121">&quot;Data&quot;</span>)
plt<span style="color: #666666">.</span>ylabel(<span style="color: #BA2121">&quot;Darget&quot;</span>)
plt<span style="color: #666666">.</span>title(<span style="color: #BA2121">&quot;Decision Tree Regression&quot;</span>)
plt<span style="color: #666666">.</span>legend()
plt<span style="color: #666666">.</span>show()
</pre></div>

<h2 id="___sec17">Concluding with another teaser, \( k \)-nearest neighbors with scikit-learn </h2>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">IPython.display</span> <span style="color: #008000; font-weight: bold">import</span> display
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">sklearn</span>
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.linear_model</span> <span style="color: #008000; font-weight: bold">import</span> LinearRegression
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.tree</span> <span style="color: #008000; font-weight: bold">import</span> DecisionTreeRegressor
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.model_selection</span> <span style="color: #008000; font-weight: bold">import</span> train_test_split
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">mglearn</span>
X, y <span style="color: #666666">=</span> mglearn<span style="color: #666666">.</span>datasets<span style="color: #666666">.</span>make_forge()
X_train, X_test, y_train, y_test <span style="color: #666666">=</span> train_test_split(X, y, random_state<span style="color: #666666">=0</span>)
<span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">sklearn.neighbors</span> <span style="color: #008000; font-weight: bold">import</span> KNeighborsClassifier
clf <span style="color: #666666">=</span> KNeighborsClassifier(n_neighbors<span style="color: #666666">=3</span>)
clf<span style="color: #666666">.</span>fit(X_train, y_train)
KNeighborsClassifier(algorithm<span style="color: #666666">=</span><span style="color: #BA2121">&#39;auto&#39;</span>, leaf_size<span style="color: #666666">=30</span>, metric<span style="color: #666666">=</span><span style="color: #BA2121">&#39;minkowski&#39;</span>,metric_params<span style="color: #666666">=</span><span style="color: #008000">None</span>, n_jobs<span style="color: #666666">=1</span>, n_neighbors<span style="color: #666666">=3</span>, p<span style="color: #666666">=2</span>,weights<span style="color: #666666">=</span><span style="color: #BA2121">&#39;uniform&#39;</span>)

clf<span style="color: #666666">.</span>predict(X_test)

clf<span style="color: #666666">.</span>score(X_test, y_test)
fig, axes <span style="color: #666666">=</span> plt<span style="color: #666666">.</span>subplots(<span style="color: #666666">1</span>, <span style="color: #666666">3</span>, figsize<span style="color: #666666">=</span>(<span style="color: #666666">10</span>, <span style="color: #666666">3</span>))
<span style="color: #008000; font-weight: bold">for</span> n_neighbors, ax <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">zip</span>([<span style="color: #666666">1</span>, <span style="color: #666666">3</span>, <span style="color: #666666">9</span>], axes):
        clf <span style="color: #666666">=</span> KNeighborsClassifier(n_neighbors<span style="color: #666666">=</span>n_neighbors)<span style="color: #666666">.</span>fit(X, y)
        mglearn<span style="color: #666666">.</span>plots<span style="color: #666666">.</span>plot_2d_separator(clf, X, fill<span style="color: #666666">=</span><span style="color: #008000">True</span>, eps<span style="color: #666666">=0.5</span>, ax<span style="color: #666666">=</span>ax, alpha<span style="color: #666666">=.4</span>)
        ax<span style="color: #666666">.</span>scatter(X[:, <span style="color: #666666">0</span>], X[:, <span style="color: #666666">1</span>], c<span style="color: #666666">=</span>y, s<span style="color: #666666">=60</span>, cmap<span style="color: #666666">=</span>mglearn<span style="color: #666666">.</span>cm2)
        ax<span style="color: #666666">.</span>set_title(<span style="color: #BA2121">&quot;</span><span style="color: #BB6688; font-weight: bold">%d</span><span style="color: #BA2121"> neighbor(s)&quot;</span> <span style="color: #666666">%</span> n_neighbors)


data <span style="color: #666666">=</span> np<span style="color: #666666">.</span>loadtxt(<span style="color: #BA2121">&#39;src/Hudson_Bay.csv&#39;</span>, delimiter<span style="color: #666666">=</span><span style="color: #BA2121">&#39;,&#39;</span>, skiprows<span style="color: #666666">=1</span>)
x <span style="color: #666666">=</span> data[:,<span style="color: #666666">0</span>]
y <span style="color: #666666">=</span> data[:,<span style="color: #666666">1</span>]
<span style="color: #408080; font-style: italic">#x_train,  y_train = train_test_split(x, y, random_state=0)</span>
line <span style="color: #666666">=</span> np<span style="color: #666666">.</span>linspace(<span style="color: #666666">1900</span>,<span style="color: #666666">1930</span>,<span style="color: #666666">1000</span>,endpoint<span style="color: #666666">=</span><span style="color: #008000">False</span>)<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>,<span style="color: #666666">1</span>)
reg <span style="color: #666666">=</span> DecisionTreeRegressor(min_samples_split<span style="color: #666666">=3</span>)<span style="color: #666666">.</span>fit(x<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>,<span style="color: #666666">1</span>),y<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>,<span style="color: #666666">1</span>))
plt<span style="color: #666666">.</span>plot(line, reg<span style="color: #666666">.</span>predict(line), label<span style="color: #666666">=</span><span style="color: #BA2121">&quot;decision tree&quot;</span>)
regline <span style="color: #666666">=</span> LinearRegression()<span style="color: #666666">.</span>fit(x<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>,<span style="color: #666666">1</span>),y<span style="color: #666666">.</span>reshape(<span style="color: #666666">-1</span>,<span style="color: #666666">1</span>))
plt<span style="color: #666666">.</span>plot(line, regline<span style="color: #666666">.</span>predict(line), label<span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Linear Regression&quot;</span>)
plt<span style="color: #666666">.</span>plot(x, y, label<span style="color: #666666">=</span> <span style="color: #BA2121">&quot;Linear Regression&quot;</span>)
plt<span style="color: #666666">.</span>show()
</pre></div>
<p>

<!-- ------------------- end of main content --------------- -->


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2019, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    

