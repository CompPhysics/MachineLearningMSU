<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods">

<title>Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Optimization problems, why?', 2, None, '___sec0'),
              ('Optimization, the central part of any Machine Learning '
               'algortithm',
               2,
               None,
               '___sec1'),
              ('Revisiting our Logistic Regression case', 2, None, '___sec2'),
              ('The equations to solve', 2, None, '___sec3'),
              ("Solving using Newton-Raphson's method", 2, None, '___sec4'),
              ("Brief reminder on Newton-Raphson's method", 2, None, '___sec5'),
              ('The equations', 2, None, '___sec6'),
              ('Simple geometric interpretation', 2, None, '___sec7'),
              ('Extending to more than one variable', 2, None, '___sec8'),
              ('Steepest descent', 2, None, '___sec9'),
              ('More on Steepest descent', 2, None, '___sec10'),
              ('The ideal', 2, None, '___sec11'),
              ('The sensitiveness of the gradient descent',
               2,
               None,
               '___sec12'),
              ('Convex functions', 2, None, '___sec13'),
              ('Convex function', 2, None, '___sec14'),
              ('Conditions on convex functions', 2, None, '___sec15'),
              ('More on convex functions', 2, None, '___sec16'),
              ('Some simple problems', 2, None, '___sec17'),
              ('Standard steepest descent', 2, None, '___sec18'),
              ('Gradient method', 2, None, '___sec19'),
              ('Steepest descent  method', 2, None, '___sec20'),
              ('Steepest descent  method', 2, None, '___sec21'),
              ('Final expressions', 2, None, '___sec22'),
              ('Code examples for steepest descent', 2, None, '___sec23'),
              ('Simple codes for  steepest descent and conjugate gradient '
               'using a $2\\times 2$ matrix, in c++, Python code to come',
               2,
               None,
               '___sec24'),
              ('The routine for the steepest descent method',
               2,
               None,
               '___sec25'),
              ('Steepest descent example', 2, None, '___sec26'),
              ('Conjugate gradient method', 2, None, '___sec27'),
              ('Conjugate gradient method', 2, None, '___sec28'),
              ('Conjugate gradient method', 2, None, '___sec29'),
              ('Conjugate gradient method', 2, None, '___sec30'),
              ('Conjugate gradient method and iterations', 2, None, '___sec31'),
              ('Conjugate gradient method', 2, None, '___sec32'),
              ('Conjugate gradient method', 2, None, '___sec33'),
              ('Conjugate gradient method', 2, None, '___sec34'),
              ('Simple implementation of the Conjugate gradient algorithm',
               2,
               None,
               '___sec35'),
              ('Broyden–Fletcher–Goldfarb–Shanno algorithm',
               2,
               None,
               '___sec36'),
              ('Revisiting our first homework', 2, None, '___sec37'),
              ('Gradient descent example', 2, None, '___sec38'),
              ('The derivative of the cost/loss function', 2, None, '___sec39'),
              ('The Hessian matrix', 2, None, '___sec40'),
              ('Simple program', 2, None, '___sec41'),
              ('Gradient Descent Example', 2, None, '___sec42'),
              ('And a corresponding example using _scikit-learn_',
               2,
               None,
               '___sec43'),
              ('Gradient descent and Ridge', 2, None, '___sec44'),
              ('Automatic differentiation', 2, None, '___sec45'),
              ('Using autograd', 2, None, '___sec46'),
              ('Autograd with more complicated functions', 2, None, '___sec47'),
              ('More complicated functions using the elements of their '
               'arguments directly',
               2,
               None,
               '___sec48'),
              ('Functions using mathematical functions from Numpy',
               2,
               None,
               '___sec49'),
              ('More autograd', 2, None, '___sec50'),
              ('And  with loops', 2, None, '___sec51'),
              ('Using recursion', 2, None, '___sec52'),
              ('Unsupported functions', 2, None, '___sec53'),
              ('The syntax a.dot(b) when finding the dot product',
               2,
               None,
               '___sec54'),
              ('Recommended to avoid', 2, None, '___sec55'),
              ('Stochastic Gradient Descent', 2, None, '___sec56'),
              ('Computation of gradients', 2, None, '___sec57'),
              ('SGD example', 2, None, '___sec58'),
              ('The gradient step', 2, None, '___sec59'),
              ('Simple example code', 2, None, '___sec60'),
              ('When do we stop?', 2, None, '___sec61'),
              ('Slightly different approach', 2, None, '___sec62'),
              ('Program for stochastic gradient', 2, None, '___sec63'),
              ('Using gradient descent methods, limitations',
               2,
               None,
               '___sec64'),
              ('Momentum based GD', 2, None, '___sec65'),
              ('More on momentum based approaches', 2, None, '___sec66'),
              ('Momentum parameter', 2, None, '___sec67'),
              ('Second moment of the gradient', 2, None, '___sec68'),
              ('RMS prop', 2, None, '___sec69'),
              ('ADAM optimizer', 2, None, '___sec70'),
              ('Practical tips', 2, None, '___sec71')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="GradientOptim-bs.html">Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs001.html#___sec0" style="font-size: 80%;">Optimization problems, why?</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs002.html#___sec1" style="font-size: 80%;">Optimization, the central part of any Machine Learning algortithm</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs003.html#___sec2" style="font-size: 80%;">Revisiting our Logistic Regression case</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs004.html#___sec3" style="font-size: 80%;">The equations to solve</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs005.html#___sec4" style="font-size: 80%;">Solving using Newton-Raphson's method</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs006.html#___sec5" style="font-size: 80%;">Brief reminder on Newton-Raphson's method</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs007.html#___sec6" style="font-size: 80%;">The equations</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs008.html#___sec7" style="font-size: 80%;">Simple geometric interpretation</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs009.html#___sec8" style="font-size: 80%;">Extending to more than one variable</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs010.html#___sec9" style="font-size: 80%;">Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs011.html#___sec10" style="font-size: 80%;">More on Steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs012.html#___sec11" style="font-size: 80%;">The ideal</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs013.html#___sec12" style="font-size: 80%;">The sensitiveness of the gradient descent</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs014.html#___sec13" style="font-size: 80%;">Convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs015.html#___sec14" style="font-size: 80%;">Convex function</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs016.html#___sec15" style="font-size: 80%;">Conditions on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs017.html#___sec16" style="font-size: 80%;">More on convex functions</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs018.html#___sec17" style="font-size: 80%;">Some simple problems</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs019.html#___sec18" style="font-size: 80%;">Standard steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs020.html#___sec19" style="font-size: 80%;">Gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs021.html#___sec20" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs022.html#___sec21" style="font-size: 80%;">Steepest descent  method</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs023.html#___sec22" style="font-size: 80%;">Final expressions</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs024.html#___sec23" style="font-size: 80%;">Code examples for steepest descent</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs025.html#___sec24" style="font-size: 80%;">Simple codes for  steepest descent and conjugate gradient using a \( 2\times 2 \) matrix, in c++, Python code to come</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs026.html#___sec25" style="font-size: 80%;">The routine for the steepest descent method</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs027.html#___sec26" style="font-size: 80%;">Steepest descent example</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs028.html#___sec27" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs029.html#___sec28" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs030.html#___sec29" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs031.html#___sec30" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs032.html#___sec31" style="font-size: 80%;">Conjugate gradient method and iterations</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs033.html#___sec32" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs034.html#___sec33" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs035.html#___sec34" style="font-size: 80%;">Conjugate gradient method</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs036.html#___sec35" style="font-size: 80%;">Simple implementation of the Conjugate gradient algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs037.html#___sec36" style="font-size: 80%;">Broyden–Fletcher–Goldfarb–Shanno algorithm</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs038.html#___sec37" style="font-size: 80%;">Revisiting our first homework</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs039.html#___sec38" style="font-size: 80%;">Gradient descent example</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs040.html#___sec39" style="font-size: 80%;">The derivative of the cost/loss function</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs041.html#___sec40" style="font-size: 80%;">The Hessian matrix</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs042.html#___sec41" style="font-size: 80%;">Simple program</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs043.html#___sec42" style="font-size: 80%;">Gradient Descent Example</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs044.html#___sec43" style="font-size: 80%;">And a corresponding example using <b>scikit-learn</b></a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs045.html#___sec44" style="font-size: 80%;">Gradient descent and Ridge</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs046.html#___sec45" style="font-size: 80%;">Automatic differentiation</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs047.html#___sec46" style="font-size: 80%;">Using autograd</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs048.html#___sec47" style="font-size: 80%;">Autograd with more complicated functions</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs049.html#___sec48" style="font-size: 80%;">More complicated functions using the elements of their arguments directly</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs050.html#___sec49" style="font-size: 80%;">Functions using mathematical functions from Numpy</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs051.html#___sec50" style="font-size: 80%;">More autograd</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs052.html#___sec51" style="font-size: 80%;">And  with loops</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs053.html#___sec52" style="font-size: 80%;">Using recursion</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs054.html#___sec53" style="font-size: 80%;">Unsupported functions</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs055.html#___sec54" style="font-size: 80%;">The syntax a.dot(b) when finding the dot product</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs056.html#___sec55" style="font-size: 80%;">Recommended to avoid</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs057.html#___sec56" style="font-size: 80%;">Stochastic Gradient Descent</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs058.html#___sec57" style="font-size: 80%;">Computation of gradients</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs059.html#___sec58" style="font-size: 80%;">SGD example</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs060.html#___sec59" style="font-size: 80%;">The gradient step</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs061.html#___sec60" style="font-size: 80%;">Simple example code</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs062.html#___sec61" style="font-size: 80%;">When do we stop?</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs063.html#___sec62" style="font-size: 80%;">Slightly different approach</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs064.html#___sec63" style="font-size: 80%;">Program for stochastic gradient</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs065.html#___sec64" style="font-size: 80%;">Using gradient descent methods, limitations</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs066.html#___sec65" style="font-size: 80%;">Momentum based GD</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs067.html#___sec66" style="font-size: 80%;">More on momentum based approaches</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs068.html#___sec67" style="font-size: 80%;">Momentum parameter</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs069.html#___sec68" style="font-size: 80%;">Second moment of the gradient</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs070.html#___sec69" style="font-size: 80%;">RMS prop</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs071.html#___sec70" style="font-size: 80%;">ADAM optimizer</a></li>
     <!-- navigation toc: --> <li><a href="._GradientOptim-bs072.html#___sec71" style="font-size: 80%;">Practical tips</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0000"></a>
<!-- ------------------- main content ---------------------- -->



<div class="jumbotron">
<center><h1>Data Analysis and Machine Learning  Lectures: Optimization and  Gradient Methods</h1></center>  <!-- document title -->

<p>
<!-- author(s): Morten Hjorth-Jensen -->

<center>
<b>Morten Hjorth-Jensen</b> [1, 2]
</center>

<p>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, University of Oslo</b></center>
<center>[2] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>
<center><h4>Mar 11, 2019</h4></center> <!-- date -->
<br>
<p>


<p><a href="._GradientOptim-bs001.html" class="btn btn-primary btn-lg">Read &raquo;</a></p>


</div> <!-- end jumbotron -->

<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pagination">
  <li class="active"><a href="._GradientOptim-bs000.html">1</a></li>
  <li><a href="._GradientOptim-bs001.html">2</a></li>
  <li><a href="._GradientOptim-bs002.html">3</a></li>
  <li><a href="._GradientOptim-bs003.html">4</a></li>
  <li><a href="._GradientOptim-bs004.html">5</a></li>
  <li><a href="._GradientOptim-bs005.html">6</a></li>
  <li><a href="._GradientOptim-bs006.html">7</a></li>
  <li><a href="._GradientOptim-bs007.html">8</a></li>
  <li><a href="._GradientOptim-bs008.html">9</a></li>
  <li><a href="._GradientOptim-bs009.html">10</a></li>
  <li><a href="">...</a></li>
  <li><a href="._GradientOptim-bs072.html">73</a></li>
  <li><a href="._GradientOptim-bs001.html">&raquo;</a></li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright --> &copy; 1999-2019, Morten Hjorth-Jensen. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    

