# Welcome to the Summer School on Machine Learning applied to Nuclear Physics, May 20-23, 2019, organized by the Facility of Rare Isotope Beams theory alliance (FRIB-TA).

### Venue: Facility of Rare Isotope Beams and National Superconducting Cyclotron Laboratory, 640 S. Shaw Lane, Michigan State University, East Lansing, 48824 Michigan

All learning material and teaching schedule pertinent to the course is avaliable at this GitHub address. A simple _git clone_ of the material gives you access to all lecture notes and program examples. Similarly, running a _git pull_ gives you immediately the latest updates. For an easy visualization of the learning material (html, jupyter-notebooks or plain pdf files), see https://compphysics.github.io/MachineLearningMSU/doc/web/course.html

## Course content

Probability theory and statistical methods play a central role in science. Nowadays we are
surrounded by huge amounts of data. For example, there are about one trillion web pages; more than one
hour of video is uploaded to YouTube every second, amounting to 10 years of content every
day; the genomes of 1000s of people, each of which has a length of more than a billion  base pairs, have
been sequenced by various labs and so on.
This deluge of data calls for automated methods of data analysis, which is exactly what machine learning provides. 
The purpose of this summer school is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists and nuclear physicists in particular. We will start with some of the basic methods from supervised learning, such as various regression methods before we move into deep learning methods for both supervised and unsupervised learning, with an emphasis on the analysis of nuclear physics experiments and theoretical nuclear physics. 
Hands-on examples will be provided and the aim is to give the participants an overview on how machine learning can be used to analyze and study nuclear physics problems. 

###  The following topics will be covered
- Basic concepts, expectation values, variance, covariance, correlation functions and errors;
- Estimation of errors using cross-validation, blocking, bootstrapping and jackknife methods;
- Optimization of functions
- Linear Regression and Logistic Regression;
- Boltzmann machines;
- Neural networks;
- Decisions trees and nearest neighbor algorithms

All the above topics will be supported by examples, hands-on exercises and project work.


## Practicalities

1. Five lectures per day Monday through Wednesday, starting at 830am, see schedule below
2. Hands-on sessions in the afternoons till 6pm
3. The last day, Thursday, is dedicated to solving explicit problems with hands-on guidance and relevant examples.


## Possible textbooks

_Recommended textbook_:
- Aurelien Geron, Hands‑On Machine Learning with Scikit‑Learn and TensorFlow, O'Reilly

_General learning book on statistical analysis_:
- Christian Robert and George Casella, Monte Carlo Statistical Methods, Springer
- Peter Hoff, A first course in Bayesian statistical models, Springer
- Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, The Elements of Statistical Learning, Springer
_General Machine Learning Books_:
- Kevin Murphy, Machine Learning: A Probabilistic Perspective, MIT Press
- Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer
- David J.C. MacKay, Information Theory, Inference, and Learning Algorithms, Cambridge University Press
- David Barber, Bayesian Reasoning and Machine Learning, Cambridge University Press 

## Schedule

Lectures are 50 min and there is a small break of 10 min between each lecture. Longer breaks at 1030am-11am and 3pm-330pm.
Acronyms for teachers
- MH = Matthew Hirn
- MHJ = Morten Hjorth-Jensen
- MK = Michelle Kuchera
- RR = Raghuram Ramanujan

_Monday May 20, 2019_:  (breaks of 10 min for every lecture)

- 8am-830am: Welcome and registration
- 830am-930am: Introduction to Machine Learning and various Python packages (MHJ)
- 930am-1030am: Linear Regression  (MHJ)
- 1030am-11am: Break, coffee, tea etc
- 11am-12pm: Logistic Regression (MHJ)
- 12pm-1pm: Lunch
- 1pm-2pm: Optimization of functions, gradient descent and stochastic gradient descent (MHJ)
- 2pm-3pm: Decision Trees and Random Forests  (MHJ)
- 3pm-330pm: Break, coffee, tea etc
- 330pm-6pm: Hands-on sessions with selected Physics examples

_Tuesday May 21, 2019_:

- 830am-930am: Neural networks  (MK and RR)
- 930am-1030am: Neural networks and deep learning  (MK and RR)
- 1030am-11am: Break, coffee, tea etc
- 11am-12pm: Convolutional Neural Networks (CNNs) and examples from nuclear physics experiments (MK and RR)
- 12pm-1pm: Lunch
- 1pm-2pm: CNNs (MK and RR)
- 2pm-3pm: Autoenconders and recurrent neural networks and examples from nuclear physics experiments  (MK and RR)
- 3pm-330pm: Break, coffee, tea etc
- 330pm-6pm: Hands-on sessions with examples from nuclear physics experiments

_Wednesday May 22, 2019_:

- 830am-930am: Reinforcement learning  (MK) 
- 930am-1030am:  Introduction to exploratory data analysis and unsupervised learning: PCA  (MH)
- 1030am-11am: Break, coffee, tea etc
- 11am-12pm: Clustering and introduction to nonlinear dimension reduction: k-means and t-SNE  (MH)
- 12pm-1pm: Lunch
- 1pm-2pm:  Nonlinear dimension reduction: Spectral graph theory and manifold learning (MH)
- 2pm-3pm:  Boltzmann machines and many-body problems  (MHJ)
- 3pm-330pm: Break, coffee, tea etc
- 3pm-6pm: Hands-on sessions with various examples

_Thursday May 23, 2019_:

- 9am-10am: Current state of Machine Learning research (MK)
- 10am-12pm: Hands-on sessions with examples from nuclear physics, experiment and theory
- Coffee, tea etc at 1030am
- 12pm-1pm: Lunch
- 1pm-5pm: Hands-on sessions with examples from nuclear physics, experiment and theory. 
- Coffee, tea, etc at 3pm
