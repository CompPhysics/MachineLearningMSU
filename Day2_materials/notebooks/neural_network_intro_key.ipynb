{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Fully Connected Neural Networks\n",
    "\n",
    "In this activity you will learn how to apply dense neural networks in Keras. \n",
    "\n",
    "The dataset that will be studied is a collection of simulated particle events from Pythia. Specifically, you will construct a dense neural network which will learn how to calculate the invariant mass of a particle from its energy and momentum.\n",
    "\n",
    "First, import numpy, tensorflow, and pylab and load the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pylab as plt\n",
    "\n",
    "# Prevent TensorFlow from showing us deprecation warnings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# MPK comp hack. You should not need this.\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = np.load(\"../data/homogenous-16-particle-events-energy.npy\")\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datset is a 2D array where each row represents one event of data from an electron-proton collision. This dataset is comprised ONLY of events where there were exactly 16 particles produced from an electron-proton collision. Each particle has an x-momentum,  y-momentum, z-momentum, energy, and charge: $(p_x,p_y,p_z,E,q)$. Each event is therefore represented by 80 numbers. Using numpy's reshape method we can make each row represent one particle.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.reshape(data,(len(data),16,5))\n",
    "print(data.shape)\n",
    "data = np.reshape(data, (len(data)*16,5))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are our training data inputs, but we also must provide the targets, which are the invariant masses of each particle. This is a straightforward computation that does not require neural networks, but provides an easy-to-understand problem.\n",
    "\n",
    "We choose units where $c = 1$:\n",
    "$$m^2=E^2-||\\textbf{p}||^2$$\n",
    "where $m, E$, and $\\textbf{p}$ are all in GeV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = data[:,0]**2 + data[:,1]**2 + data[:,2]**2\n",
    "target = np.sqrt(np.maximum(data[:,3]**2 - p2,0))\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several hundred thousand datapoints in this dataset which is overkill for this simple example. Create a test dataset with just 1000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallTarget=target[:1000]\n",
    "smallEnergy=data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, make a histogram of the target data to make sure that we are seeing masses of real particles. As this data has limited precision, this will not resolve electrons very well, but protons, pions, and massless particles should be clearly visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(target,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build and train the first neural network. Start with a simple one hidden layer network with 5 neurons, and ReLU activation. Train for 30 epochs with a batch size of 256, an Adam optimizer with a learning rate of 0.1, using mean squared error loss. \n",
    "\n",
    "Additionally, use a validation split of 0.8 when fitting to check for overtraining. Information on how to implement these features can be found here:\n",
    "<https://www.tensorflow.org/api_docs/python/tf/keras>.\n",
    "Check out Sequential underneath models and Dense under layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(5,input_shape=(5,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(5,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.1),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "results = model.fit(smallEnergy,smallTarget,epochs=30, batch_size=256, validation_split=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = results.history\n",
    "print(target[-10:])\n",
    "y = model.predict(data[-10:])\n",
    "print(y)\n",
    "plt.plot(history[\"loss\"], label=\"training loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is learning, but we can do better. Try increasing the number of hidden neurons to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(5,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(20,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.1),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "model.fit(smallEnergy,smallTarget,epochs=30, batch_size=256, validation_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did not do much better, try decreasing the batch size to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(5,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(20,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.1),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "model.fit(smallEnergy,smallTarget,epochs=30, batch_size=16, validation_split=0.8)\n",
    "print(target[-10:])\n",
    "y = model.predict(data[-10:])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to do better but has room for improvemnt. Perhpas the learning rate is to high and the network can't fine tune. Try decreasing the learning rate to 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(5,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(20,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.001),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "model.fit(smallEnergy,smallTarget,epochs=30, batch_size=16, validation_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not really that mutch better, but now there is evidence of overtraining, as the training loss is so much lower than the validation loss. A common fix to this is adding dropout layers. Try adding a dropout layer with dropout rate of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(5,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(20,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.001),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "model.fit(smallEnergy,smallTarget,epochs=30, batch_size=16, validation_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly stoped the overtraining problem, but it still isn't training well. Now, try training on the full dataset with a more reasonable validation split of 0.2. Use the a single hidden layer with 20 neurons, a learning rate of 0.001, and a batch size of 256. Just run it for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(5,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(20,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.001),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "model.fit(data,target,epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resulted in significant improvement and shows how important having a large enough dataset it. Moving on to the choice in activation functions, ReLU is not the only available choice, although it is currently the most popular for deep networks. \n",
    "\n",
    "Train a network using a sigmoid and/or tanh activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(5,),activation=\"sigmoid\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(20,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.001),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "model.fit(data,target,epochs=10, batch_size=256, validation_split=0.2)\n",
    "\n",
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(5,),activation=\"tanh\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(20,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.001),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "model.fit(data,target,epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try adding 2 new hidden layers to the network. Use the relu activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(5,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(20,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(20,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(20,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.001),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "model.fit(data,target,epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, adding more layers helps improve the quality of the network. There is a limit to now effective this is though. Try having 5 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(5,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(20,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(20,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(20,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(20,input_shape=(20,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(20,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.001),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "model.fit(energy,target,epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, see what happens when you increase the number of neurons per layer from 20 to 50 in the 3 hidden layer model. Consider how they perform compared to Relu now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(50,input_shape=(5,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(50,input_shape=(50,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(50,input_shape=(50,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(50,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.001),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "model.fit(energy,target,epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using the sigmoid and the tanh activation functions again and compare them to relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(50,input_shape=(5,),activation=\"sigmoid\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(50,input_shape=(50,),activation=\"sigmoid\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(50,input_shape=(50,),activation=\"sigmoid\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(50,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.001),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "model.fit(energy,target,epochs=10, batch_size=256, validation_split=0.2)\n",
    "\n",
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(50,input_shape=(5,),activation=\"tanh\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(50,input_shape=(50,),activation=\"tanh\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(50,input_shape=(50,),activation=\"tanh\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(50,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.001),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "model.fit(energy,target,epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This difference in performace, especially with the sigmoid function, is known as the vanishing gradient problem. If the value for any one the neurons gets too far away from 0, the gradient for sigmoid and tanh gets really close to 0. This means that for deeper networks it is much more difficult to update the first layers as their gradient is so small. Now, remove the fifth column from the input data, the charge, and see what happens when training. Why do you thing including charge has such a large impact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential() #Define the model object\n",
    "model.add(tf.keras.layers.Dense(50,input_shape=(5,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(50,input_shape=(50,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(50,input_shape=(50,),activation=\"relu\")) #Add the hidden layer\n",
    "model.add(tf.keras.layers.Dense(1,input_shape=(50,))) #Add the output layer\n",
    "model.compile(tf.keras.optimizers.Adam(lr=0.001),loss=tf.keras.losses.MeanSquaredError()) #Adam optimizer and mean squared error loss\n",
    "model.fit(data,target,epochs=10, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there are other options for the loss function. Try experimenting with mean squared error. Other optimizers that can be used are sgd, rmsprop, adagrad, adadelta, adamax, and nadam. <https://www.tensorflow.org/api_docs/python/tf/keras/optimizers>\n",
    "\n",
    "<https://www.tensorflow.org/api_docs/python/tf/keras/losses>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(data[-100000:])\n",
    "plt.hist(y,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
